17/08/29 11:58:29 INFO SparkContext: Running Spark version 2.2.0
17/08/29 11:58:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/08/29 11:58:30 INFO SparkContext: Submitted application: W2V
17/08/29 11:58:30 INFO SecurityManager: Changing view acls to: centos
17/08/29 11:58:30 INFO SecurityManager: Changing modify acls to: centos
17/08/29 11:58:30 INFO SecurityManager: Changing view acls groups to: 
17/08/29 11:58:30 INFO SecurityManager: Changing modify acls groups to: 
17/08/29 11:58:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(centos); groups with view permissions: Set(); users  with modify permissions: Set(centos); groups with modify permissions: Set()
17/08/29 11:58:30 INFO Utils: Successfully started service 'sparkDriver' on port 44957.
17/08/29 11:58:30 INFO SparkEnv: Registering MapOutputTracker
17/08/29 11:58:30 INFO SparkEnv: Registering BlockManagerMaster
17/08/29 11:58:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/08/29 11:58:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/08/29 11:58:30 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1d2139c6-96a0-4d1a-98b8-8323a1e42371
17/08/29 11:58:30 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/08/29 11:58:30 INFO SparkEnv: Registering OutputCommitCoordinator
17/08/29 11:58:31 INFO log: Logging initialized @3615ms
17/08/29 11:58:31 INFO Server: jetty-9.3.z-SNAPSHOT
17/08/29 11:58:31 INFO Server: Started @3801ms
17/08/29 11:58:31 INFO AbstractConnector: Started ServerConnector@352c308{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17/08/29 11:58:31 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/08/29 11:58:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@72efb5c1{/jobs,null,AVAILABLE,@Spark}
17/08/29 11:58:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2449cff7{/jobs/json,null,AVAILABLE,@Spark}
17/08/29 11:58:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@62da83ed{/jobs/job,null,AVAILABLE,@Spark}
17/08/29 11:58:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@384fc774{/jobs/job/json,null,AVAILABLE,@Spark}
17/08/29 11:58:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@71e9a896{/stages,null,AVAILABLE,@Spark}
17/08/29 11:58:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@408b35bf{/stages/json,null,AVAILABLE,@Spark}
17/08/29 11:58:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@15bcf458{/stages/stage,null,AVAILABLE,@Spark}
17/08/29 11:58:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@726386ed{/stages/stage/json,null,AVAILABLE,@Spark}
17/08/29 11:58:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@14bb2297{/stages/pool,null,AVAILABLE,@Spark}
17/08/29 11:58:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@797501a{/stages/pool/json,null,AVAILABLE,@Spark}
17/08/29 11:58:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@57f791c6{/storage,null,AVAILABLE,@Spark}
17/08/29 11:58:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6c4f9535{/storage/json,null,AVAILABLE,@Spark}
17/08/29 11:58:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@30c31dd7{/storage/rdd,null,AVAILABLE,@Spark}
17/08/29 11:58:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@596df867{/storage/rdd/json,null,AVAILABLE,@Spark}
17/08/29 11:58:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@241a53ef{/environment,null,AVAILABLE,@Spark}
17/08/29 11:58:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2db2cd5{/environment/json,null,AVAILABLE,@Spark}
17/08/29 11:58:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@615f972{/executors,null,AVAILABLE,@Spark}
17/08/29 11:58:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@73393584{/executors/json,null,AVAILABLE,@Spark}
17/08/29 11:58:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1827a871{/executors/threadDump,null,AVAILABLE,@Spark}
17/08/29 11:58:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7249dadf{/executors/threadDump/json,null,AVAILABLE,@Spark}
17/08/29 11:58:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@66238be2{/static,null,AVAILABLE,@Spark}
17/08/29 11:58:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5ea502e0{/,null,AVAILABLE,@Spark}
17/08/29 11:58:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@473b3b7a{/api,null,AVAILABLE,@Spark}
17/08/29 11:58:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@133e019b{/jobs/job/kill,null,AVAILABLE,@Spark}
17/08/29 11:58:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7dac3fd8{/stages/stage/kill,null,AVAILABLE,@Spark}
17/08/29 11:58:31 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.0.15:4040
17/08/29 11:58:31 INFO SparkContext: Added JAR file:/home/centos/kjzth/sparksample/target/scala-2.10/spark-sample_2.10-1.0.jar at spark://10.0.0.15:44957/jars/spark-sample_2.10-1.0.jar with timestamp 1503982711529
17/08/29 11:58:31 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://findfeatures-hra-master-0.novalocal:7077...
17/08/29 11:58:31 INFO TransportClientFactory: Successfully created connection to findfeatures-hra-master-0.novalocal/10.0.0.15:7077 after 29 ms (0 ms spent in bootstraps)
17/08/29 11:58:31 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170829115831-0139
17/08/29 11:58:31 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/0 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 11:58:31 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/0 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 11:58:31 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35768.
17/08/29 11:58:31 INFO NettyBlockTransferService: Server created on 10.0.0.15:35768
17/08/29 11:58:31 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/08/29 11:58:31 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/0 is now RUNNING
17/08/29 11:58:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.0.15, 35768, None)
17/08/29 11:58:31 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.0.15:35768 with 366.3 MB RAM, BlockManagerId(driver, 10.0.0.15, 35768, None)
17/08/29 11:58:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.0.15, 35768, None)
17/08/29 11:58:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.0.15, 35768, None)
17/08/29 11:58:32 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5c089b2f{/metrics/json,null,AVAILABLE,@Spark}
17/08/29 11:58:33 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.0.10:53254) with ID 0
17/08/29 11:58:33 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.0.10:38807 with 1295.4 MB RAM, BlockManagerId(0, 10.0.0.10, 38807, None)
17/08/29 11:58:34 INFO EventLoggingListener: Logging events to hdfs:///tmp/spark-events/app-20170829115831-0139
17/08/29 11:58:34 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/08/29 11:58:34 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 238.5 KB, free 366.1 MB)
17/08/29 11:58:35 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.2 KB, free 366.0 MB)
17/08/29 11:58:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.0.15:35768 (size: 23.2 KB, free: 366.3 MB)
17/08/29 11:58:35 INFO SparkContext: Created broadcast 0 from textFile at W2V.scala:9
17/08/29 11:58:35 INFO FileInputFormat: Total input paths to process : 1
17/08/29 11:58:35 INFO SparkContext: Starting job: collect at Word2Vec.scala:196
17/08/29 11:58:35 INFO DAGScheduler: Registering RDD 4 (map at Word2Vec.scala:187)
17/08/29 11:58:35 INFO DAGScheduler: Got job 0 (collect at Word2Vec.scala:196) with 20 output partitions
17/08/29 11:58:35 INFO DAGScheduler: Final stage: ResultStage 1 (collect at Word2Vec.scala:196)
17/08/29 11:58:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
17/08/29 11:58:35 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
17/08/29 11:58:35 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at map at Word2Vec.scala:187), which has no missing parents
17/08/29 11:58:35 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.9 KB, free 366.0 MB)
17/08/29 11:58:35 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.8 KB, free 366.0 MB)
17/08/29 11:58:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.0.15:35768 (size: 2.8 KB, free: 366.3 MB)
17/08/29 11:58:35 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
17/08/29 11:58:35 INFO DAGScheduler: Submitting 20 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at map at Word2Vec.scala:187) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
17/08/29 11:58:35 INFO TaskSchedulerImpl: Adding task set 0.0 with 20 tasks
17/08/29 11:58:35 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10.0.0.10, executor 0, partition 0, ANY, 4877 bytes)
17/08/29 11:58:36 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 10.0.0.10, executor 0, partition 1, ANY, 4877 bytes)
17/08/29 11:58:36 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.0.10:38807 (size: 2.8 KB, free: 1295.4 MB)
17/08/29 11:58:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.0.10:38807 (size: 23.2 KB, free: 1295.4 MB)
17/08/29 11:58:45 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, 10.0.0.10, executor 0, partition 2, ANY, 4877 bytes)
17/08/29 11:58:45 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 9410 ms on 10.0.0.10 (executor 0) (1/20)
17/08/29 11:58:45 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, 10.0.0.10, executor 0, partition 3, ANY, 4877 bytes)
17/08/29 11:58:45 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 9419 ms on 10.0.0.10 (executor 0) (2/20)
17/08/29 11:58:50 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, 10.0.0.10, executor 0, partition 4, ANY, 4877 bytes)
17/08/29 11:58:50 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 4806 ms on 10.0.0.10 (executor 0) (3/20)
17/08/29 11:58:51 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, 10.0.0.10, executor 0, partition 5, ANY, 4877 bytes)
17/08/29 11:58:51 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 5867 ms on 10.0.0.10 (executor 0) (4/20)
17/08/29 11:58:54 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, 10.0.0.10, executor 0, partition 6, ANY, 4877 bytes)
17/08/29 11:58:54 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 4410 ms on 10.0.0.10 (executor 0) (5/20)
17/08/29 11:58:55 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, 10.0.0.10, executor 0, partition 7, ANY, 4877 bytes)
17/08/29 11:58:55 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 4417 ms on 10.0.0.10 (executor 0) (6/20)
17/08/29 11:58:59 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, 10.0.0.10, executor 0, partition 8, ANY, 4877 bytes)
17/08/29 11:58:59 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 4571 ms on 10.0.0.10 (executor 0) (7/20)
17/08/29 11:59:00 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, 10.0.0.10, executor 0, partition 9, ANY, 4877 bytes)
17/08/29 11:59:00 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 5232 ms on 10.0.0.10 (executor 0) (8/20)
17/08/29 11:59:03 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10, 10.0.0.10, executor 0, partition 10, ANY, 4877 bytes)
17/08/29 11:59:03 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 4706 ms on 10.0.0.10 (executor 0) (9/20)
17/08/29 11:59:04 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11, 10.0.0.10, executor 0, partition 11, ANY, 4877 bytes)
17/08/29 11:59:04 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 3728 ms on 10.0.0.10 (executor 0) (10/20)
17/08/29 11:59:08 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12, 10.0.0.10, executor 0, partition 12, ANY, 4877 bytes)
17/08/29 11:59:08 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 5094 ms on 10.0.0.10 (executor 0) (11/20)
17/08/29 11:59:09 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13, 10.0.0.10, executor 0, partition 13, ANY, 4877 bytes)
17/08/29 11:59:09 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 5094 ms on 10.0.0.10 (executor 0) (12/20)
17/08/29 11:59:13 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14, 10.0.0.10, executor 0, partition 14, ANY, 4877 bytes)
17/08/29 11:59:13 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 4471 ms on 10.0.0.10 (executor 0) (13/20)
17/08/29 11:59:13 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15, 10.0.0.10, executor 0, partition 15, ANY, 4877 bytes)
17/08/29 11:59:13 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 4168 ms on 10.0.0.10 (executor 0) (14/20)
17/08/29 11:59:17 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16, 10.0.0.10, executor 0, partition 16, ANY, 4877 bytes)
17/08/29 11:59:17 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 4215 ms on 10.0.0.10 (executor 0) (15/20)
17/08/29 11:59:18 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17, 10.0.0.10, executor 0, partition 17, ANY, 4877 bytes)
17/08/29 11:59:18 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 4772 ms on 10.0.0.10 (executor 0) (16/20)
17/08/29 11:59:21 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18, 10.0.0.10, executor 0, partition 18, ANY, 4877 bytes)
17/08/29 11:59:21 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 4091 ms on 10.0.0.10 (executor 0) (17/20)
17/08/29 11:59:22 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19, 10.0.0.10, executor 0, partition 19, ANY, 4877 bytes)
17/08/29 11:59:22 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 4002 ms on 10.0.0.10 (executor 0) (18/20)
17/08/29 11:59:23 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 676 ms on 10.0.0.10 (executor 0) (19/20)
17/08/29 11:59:25 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 3613 ms on 10.0.0.10 (executor 0) (20/20)
17/08/29 11:59:25 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/08/29 11:59:25 INFO DAGScheduler: ShuffleMapStage 0 (map at Word2Vec.scala:187) finished in 49.352 s
17/08/29 11:59:25 INFO DAGScheduler: looking for newly runnable stages
17/08/29 11:59:25 INFO DAGScheduler: running: Set()
17/08/29 11:59:25 INFO DAGScheduler: waiting: Set(ResultStage 1)
17/08/29 11:59:25 INFO DAGScheduler: failed: Set()
17/08/29 11:59:25 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at map at Word2Vec.scala:190), which has no missing parents
17/08/29 11:59:25 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 4.8 KB, free 366.0 MB)
17/08/29 11:59:25 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 366.0 MB)
17/08/29 11:59:25 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.0.15:35768 (size: 2.6 KB, free: 366.3 MB)
17/08/29 11:59:25 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
17/08/29 11:59:25 INFO DAGScheduler: Submitting 20 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at map at Word2Vec.scala:190) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
17/08/29 11:59:25 INFO TaskSchedulerImpl: Adding task set 1.0 with 20 tasks
17/08/29 11:59:25 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 20, 10.0.0.10, executor 0, partition 0, NODE_LOCAL, 4625 bytes)
17/08/29 11:59:25 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 21, 10.0.0.10, executor 0, partition 1, NODE_LOCAL, 4625 bytes)
17/08/29 11:59:25 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.0.10:38807 (size: 2.6 KB, free: 1295.4 MB)
17/08/29 11:59:25 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.0.10:53254
17/08/29 11:59:25 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 208 bytes
17/08/29 11:59:27 INFO BlockManagerInfo: Added taskresult_20 in memory on 10.0.0.10:38807 (size: 4.2 MB, free: 1291.2 MB)
17/08/29 11:59:27 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 22, 10.0.0.10, executor 0, partition 2, NODE_LOCAL, 4625 bytes)
17/08/29 11:59:27 INFO TransportClientFactory: Successfully created connection to /10.0.0.10:38807 after 14 ms (0 ms spent in bootstraps)
17/08/29 11:59:27 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.0.15:35768 in memory (size: 2.8 KB, free: 366.3 MB)
17/08/29 11:59:27 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.0.10:38807 in memory (size: 2.8 KB, free: 1291.2 MB)
17/08/29 11:59:27 INFO BlockManagerInfo: Added taskresult_21 in memory on 10.0.0.10:38807 (size: 4.1 MB, free: 1287.1 MB)
17/08/29 11:59:27 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 23, 10.0.0.10, executor 0, partition 3, NODE_LOCAL, 4625 bytes)
17/08/29 11:59:27 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 21) in 2212 ms on 10.0.0.10 (executor 0) (1/20)
17/08/29 11:59:27 INFO BlockManagerInfo: Removed taskresult_21 on 10.0.0.10:38807 in memory (size: 4.1 MB, free: 1291.2 MB)
17/08/29 11:59:27 INFO BlockManagerInfo: Removed taskresult_20 on 10.0.0.10:38807 in memory (size: 4.2 MB, free: 1295.4 MB)
17/08/29 11:59:27 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 20) in 2244 ms on 10.0.0.10 (executor 0) (2/20)
17/08/29 11:59:29 INFO BlockManagerInfo: Added taskresult_22 in memory on 10.0.0.10:38807 (size: 4.1 MB, free: 1291.2 MB)
17/08/29 11:59:29 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 24, 10.0.0.10, executor 0, partition 4, NODE_LOCAL, 4625 bytes)
17/08/29 11:59:29 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 22) in 1864 ms on 10.0.0.10 (executor 0) (3/20)
17/08/29 11:59:29 INFO BlockManagerInfo: Removed taskresult_22 on 10.0.0.10:38807 in memory (size: 4.1 MB, free: 1295.4 MB)
17/08/29 11:59:29 INFO BlockManagerInfo: Added taskresult_23 in memory on 10.0.0.10:38807 (size: 4.2 MB, free: 1291.2 MB)
17/08/29 11:59:29 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 25, 10.0.0.10, executor 0, partition 5, NODE_LOCAL, 4625 bytes)
17/08/29 11:59:29 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 23) in 1847 ms on 10.0.0.10 (executor 0) (4/20)
17/08/29 11:59:29 INFO BlockManagerInfo: Removed taskresult_23 on 10.0.0.10:38807 in memory (size: 4.2 MB, free: 1295.4 MB)
17/08/29 11:59:30 INFO BlockManagerInfo: Added taskresult_24 in memory on 10.0.0.10:38807 (size: 4.2 MB, free: 1291.2 MB)
17/08/29 11:59:30 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 26, 10.0.0.10, executor 0, partition 6, NODE_LOCAL, 4625 bytes)
17/08/29 11:59:30 INFO BlockManagerInfo: Added taskresult_25 in memory on 10.0.0.10:38807 (size: 4.2 MB, free: 1287.0 MB)
17/08/29 11:59:30 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 24) in 1506 ms on 10.0.0.10 (executor 0) (5/20)
17/08/29 11:59:30 INFO BlockManagerInfo: Removed taskresult_24 on 10.0.0.10:38807 in memory (size: 4.2 MB, free: 1291.2 MB)
17/08/29 11:59:30 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 27, 10.0.0.10, executor 0, partition 7, NODE_LOCAL, 4625 bytes)
17/08/29 11:59:30 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 25) in 1448 ms on 10.0.0.10 (executor 0) (6/20)
17/08/29 11:59:30 INFO BlockManagerInfo: Removed taskresult_25 on 10.0.0.10:38807 in memory (size: 4.2 MB, free: 1295.4 MB)
17/08/29 11:59:31 INFO BlockManagerInfo: Added taskresult_26 in memory on 10.0.0.10:38807 (size: 4.2 MB, free: 1291.2 MB)
17/08/29 11:59:31 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 28, 10.0.0.10, executor 0, partition 8, NODE_LOCAL, 4625 bytes)
17/08/29 11:59:31 INFO BlockManagerInfo: Removed taskresult_26 on 10.0.0.10:38807 in memory (size: 4.2 MB, free: 1295.4 MB)
17/08/29 11:59:31 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 26) in 1156 ms on 10.0.0.10 (executor 0) (7/20)
17/08/29 11:59:31 INFO BlockManagerInfo: Added taskresult_27 in memory on 10.0.0.10:38807 (size: 4.1 MB, free: 1291.2 MB)
17/08/29 11:59:31 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 29, 10.0.0.10, executor 0, partition 9, NODE_LOCAL, 4625 bytes)
17/08/29 11:59:31 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 27) in 1218 ms on 10.0.0.10 (executor 0) (8/20)
17/08/29 11:59:31 INFO BlockManagerInfo: Removed taskresult_27 on 10.0.0.10:38807 in memory (size: 4.1 MB, free: 1295.4 MB)
17/08/29 11:59:32 INFO BlockManagerInfo: Added taskresult_28 in memory on 10.0.0.10:38807 (size: 4.2 MB, free: 1291.2 MB)
17/08/29 11:59:32 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 30, 10.0.0.10, executor 0, partition 10, NODE_LOCAL, 4625 bytes)
17/08/29 11:59:32 INFO BlockManagerInfo: Removed taskresult_28 on 10.0.0.10:38807 in memory (size: 4.2 MB, free: 1295.4 MB)
17/08/29 11:59:32 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 28) in 1252 ms on 10.0.0.10 (executor 0) (9/20)
17/08/29 11:59:32 INFO BlockManagerInfo: Added taskresult_29 in memory on 10.0.0.10:38807 (size: 4.2 MB, free: 1291.2 MB)
17/08/29 11:59:32 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 31, 10.0.0.10, executor 0, partition 11, NODE_LOCAL, 4625 bytes)
17/08/29 11:59:32 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 29) in 1062 ms on 10.0.0.10 (executor 0) (10/20)
17/08/29 11:59:32 INFO BlockManagerInfo: Removed taskresult_29 on 10.0.0.10:38807 in memory (size: 4.2 MB, free: 1295.4 MB)
17/08/29 11:59:33 INFO BlockManagerInfo: Added taskresult_30 in memory on 10.0.0.10:38807 (size: 4.2 MB, free: 1291.2 MB)
17/08/29 11:59:33 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 32, 10.0.0.10, executor 0, partition 12, NODE_LOCAL, 4625 bytes)
17/08/29 11:59:33 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 30) in 1225 ms on 10.0.0.10 (executor 0) (11/20)
17/08/29 11:59:33 INFO BlockManagerInfo: Removed taskresult_30 on 10.0.0.10:38807 in memory (size: 4.2 MB, free: 1295.4 MB)
17/08/29 11:59:33 INFO BlockManagerInfo: Added taskresult_31 in memory on 10.0.0.10:38807 (size: 4.2 MB, free: 1291.2 MB)
17/08/29 11:59:33 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 33, 10.0.0.10, executor 0, partition 13, NODE_LOCAL, 4625 bytes)
17/08/29 11:59:33 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 31) in 1106 ms on 10.0.0.10 (executor 0) (12/20)
17/08/29 11:59:33 INFO BlockManagerInfo: Removed taskresult_31 on 10.0.0.10:38807 in memory (size: 4.2 MB, free: 1295.4 MB)
17/08/29 11:59:34 INFO BlockManagerInfo: Added taskresult_32 in memory on 10.0.0.10:38807 (size: 4.2 MB, free: 1291.1 MB)
17/08/29 11:59:34 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 34, 10.0.0.10, executor 0, partition 14, NODE_LOCAL, 4625 bytes)
17/08/29 11:59:34 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 32) in 1086 ms on 10.0.0.10 (executor 0) (13/20)
17/08/29 11:59:34 INFO BlockManagerInfo: Removed taskresult_32 on 10.0.0.10:38807 in memory (size: 4.2 MB, free: 1295.4 MB)
17/08/29 11:59:34 INFO BlockManagerInfo: Added taskresult_33 in memory on 10.0.0.10:38807 (size: 4.2 MB, free: 1291.2 MB)
17/08/29 11:59:34 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 35, 10.0.0.10, executor 0, partition 15, NODE_LOCAL, 4625 bytes)
17/08/29 11:59:34 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 33) in 1019 ms on 10.0.0.10 (executor 0) (14/20)
17/08/29 11:59:34 INFO BlockManagerInfo: Removed taskresult_33 on 10.0.0.10:38807 in memory (size: 4.2 MB, free: 1295.4 MB)
17/08/29 11:59:35 INFO BlockManagerInfo: Added taskresult_34 in memory on 10.0.0.10:38807 (size: 4.1 MB, free: 1291.2 MB)
17/08/29 11:59:35 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 36, 10.0.0.10, executor 0, partition 16, NODE_LOCAL, 4625 bytes)
17/08/29 11:59:35 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 34) in 1125 ms on 10.0.0.10 (executor 0) (15/20)
17/08/29 11:59:35 INFO BlockManagerInfo: Removed taskresult_34 on 10.0.0.10:38807 in memory (size: 4.1 MB, free: 1295.4 MB)
17/08/29 11:59:35 INFO BlockManagerInfo: Added taskresult_35 in memory on 10.0.0.10:38807 (size: 4.2 MB, free: 1291.1 MB)
17/08/29 11:59:35 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 37, 10.0.0.10, executor 0, partition 17, NODE_LOCAL, 4625 bytes)
17/08/29 11:59:35 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 35) in 1148 ms on 10.0.0.10 (executor 0) (16/20)
17/08/29 11:59:35 INFO BlockManagerInfo: Removed taskresult_35 on 10.0.0.10:38807 in memory (size: 4.2 MB, free: 1295.4 MB)
17/08/29 11:59:36 INFO BlockManagerInfo: Added taskresult_36 in memory on 10.0.0.10:38807 (size: 4.2 MB, free: 1291.2 MB)
17/08/29 11:59:36 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 38, 10.0.0.10, executor 0, partition 18, NODE_LOCAL, 4625 bytes)
17/08/29 11:59:36 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 36) in 1069 ms on 10.0.0.10 (executor 0) (17/20)
17/08/29 11:59:36 INFO BlockManagerInfo: Removed taskresult_36 on 10.0.0.10:38807 in memory (size: 4.2 MB, free: 1295.4 MB)
17/08/29 11:59:36 INFO BlockManagerInfo: Added taskresult_37 in memory on 10.0.0.10:38807 (size: 4.1 MB, free: 1291.2 MB)
17/08/29 11:59:36 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 39, 10.0.0.10, executor 0, partition 19, NODE_LOCAL, 4625 bytes)
17/08/29 11:59:36 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 37) in 982 ms on 10.0.0.10 (executor 0) (18/20)
17/08/29 11:59:36 INFO BlockManagerInfo: Removed taskresult_37 on 10.0.0.10:38807 in memory (size: 4.1 MB, free: 1295.4 MB)
17/08/29 11:59:37 INFO BlockManagerInfo: Added taskresult_38 in memory on 10.0.0.10:38807 (size: 4.2 MB, free: 1291.2 MB)
17/08/29 11:59:37 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 38) in 1004 ms on 10.0.0.10 (executor 0) (19/20)
17/08/29 11:59:37 INFO BlockManagerInfo: Removed taskresult_38 on 10.0.0.10:38807 in memory (size: 4.2 MB, free: 1295.4 MB)
17/08/29 11:59:37 INFO BlockManagerInfo: Added taskresult_39 in memory on 10.0.0.10:38807 (size: 4.2 MB, free: 1291.1 MB)
17/08/29 11:59:37 INFO BlockManagerInfo: Removed taskresult_39 on 10.0.0.10:38807 in memory (size: 4.2 MB, free: 1295.4 MB)
17/08/29 11:59:37 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 39) in 1033 ms on 10.0.0.10 (executor 0) (20/20)
17/08/29 11:59:37 INFO DAGScheduler: ResultStage 1 (collect at Word2Vec.scala:196) finished in 12.544 s
17/08/29 11:59:37 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/08/29 11:59:37 INFO DAGScheduler: Job 0 finished: collect at Word2Vec.scala:196, took 62.470310 s
17/08/29 11:59:38 INFO Word2Vec: vocabSize = 208707, trainWordsCount = 33057090
17/08/29 11:59:38 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 3.9 KB, free 366.0 MB)
17/08/29 11:59:38 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.0 KB, free 366.0 MB)
17/08/29 11:59:38 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.0.15:35768 (size: 4.0 KB, free: 366.3 MB)
17/08/29 11:59:38 INFO SparkContext: Created broadcast 3 from broadcast at Word2Vec.scala:315
17/08/29 11:59:38 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 98.3 MB, free 267.7 MB)
17/08/29 11:59:40 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.0.15:35768 in memory (size: 2.6 KB, free: 366.3 MB)
17/08/29 11:59:40 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.0.10:38807 in memory (size: 2.6 KB, free: 1295.4 MB)
17/08/29 11:59:40 INFO ContextCleaner: Cleaned shuffle 0
17/08/29 11:59:40 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.0 MB, free 263.7 MB)
17/08/29 11:59:40 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.0.15:35768 (size: 4.0 MB, free: 362.3 MB)
17/08/29 11:59:40 INFO MemoryStore: Block broadcast_4_piece1 stored as bytes in memory (estimated size 4.0 MB, free 259.7 MB)
17/08/29 11:59:40 INFO BlockManagerInfo: Added broadcast_4_piece1 in memory on 10.0.0.15:35768 (size: 4.0 MB, free: 358.3 MB)
17/08/29 11:59:40 INFO MemoryStore: Block broadcast_4_piece2 stored as bytes in memory (estimated size 4.0 MB, free 255.7 MB)
17/08/29 11:59:40 INFO BlockManagerInfo: Added broadcast_4_piece2 in memory on 10.0.0.15:35768 (size: 4.0 MB, free: 354.3 MB)
17/08/29 11:59:40 INFO MemoryStore: Block broadcast_4_piece3 stored as bytes in memory (estimated size 1002.6 KB, free 254.7 MB)
17/08/29 11:59:40 INFO BlockManagerInfo: Added broadcast_4_piece3 in memory on 10.0.0.15:35768 (size: 1002.6 KB, free: 353.3 MB)
17/08/29 11:59:40 INFO SparkContext: Created broadcast 4 from broadcast at Word2Vec.scala:316
17/08/29 11:59:40 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 22.0 MB, free 232.8 MB)
17/08/29 11:59:41 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 MB, free 228.8 MB)
17/08/29 11:59:41 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.0.15:35768 (size: 4.0 MB, free: 349.3 MB)
17/08/29 11:59:41 INFO MemoryStore: Block broadcast_5_piece1 stored as bytes in memory (estimated size 3.2 MB, free 225.5 MB)
17/08/29 11:59:41 INFO BlockManagerInfo: Added broadcast_5_piece1 in memory on 10.0.0.15:35768 (size: 3.2 MB, free: 346.1 MB)
17/08/29 11:59:41 INFO SparkContext: Created broadcast 5 from broadcast at Word2Vec.scala:317
17/08/29 11:59:41 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 79.6 MB, free 145.9 MB)
17/08/29 11:59:42 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.0 MB, free 141.9 MB)
17/08/29 11:59:42 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.0.15:35768 (size: 4.0 MB, free: 342.1 MB)
17/08/29 11:59:42 INFO MemoryStore: Block broadcast_6_piece1 stored as bytes in memory (estimated size 4.0 MB, free 137.9 MB)
17/08/29 11:59:42 INFO BlockManagerInfo: Added broadcast_6_piece1 in memory on 10.0.0.15:35768 (size: 4.0 MB, free: 338.1 MB)
17/08/29 11:59:42 INFO MemoryStore: Block broadcast_6_piece2 stored as bytes in memory (estimated size 4.0 MB, free 133.9 MB)
17/08/29 11:59:42 INFO BlockManagerInfo: Added broadcast_6_piece2 in memory on 10.0.0.15:35768 (size: 4.0 MB, free: 334.1 MB)
17/08/29 11:59:42 INFO MemoryStore: Block broadcast_6_piece3 stored as bytes in memory (estimated size 4.0 MB, free 129.9 MB)
17/08/29 11:59:42 INFO BlockManagerInfo: Added broadcast_6_piece3 in memory on 10.0.0.15:35768 (size: 4.0 MB, free: 330.1 MB)
17/08/29 11:59:42 INFO MemoryStore: Block broadcast_6_piece4 stored as bytes in memory (estimated size 4.0 MB, free 125.9 MB)
17/08/29 11:59:42 INFO BlockManagerInfo: Added broadcast_6_piece4 in memory on 10.0.0.15:35768 (size: 4.0 MB, free: 326.1 MB)
17/08/29 11:59:42 INFO MemoryStore: Block broadcast_6_piece5 stored as bytes in memory (estimated size 4.0 MB, free 121.9 MB)
17/08/29 11:59:42 INFO BlockManagerInfo: Added broadcast_6_piece5 in memory on 10.0.0.15:35768 (size: 4.0 MB, free: 322.1 MB)
17/08/29 11:59:42 INFO MemoryStore: Block broadcast_6_piece6 stored as bytes in memory (estimated size 4.0 MB, free 117.9 MB)
17/08/29 11:59:42 INFO BlockManagerInfo: Added broadcast_6_piece6 in memory on 10.0.0.15:35768 (size: 4.0 MB, free: 318.1 MB)
17/08/29 11:59:42 INFO MemoryStore: Block broadcast_6_piece7 stored as bytes in memory (estimated size 4.0 MB, free 113.9 MB)
17/08/29 11:59:42 INFO BlockManagerInfo: Added broadcast_6_piece7 in memory on 10.0.0.15:35768 (size: 4.0 MB, free: 314.1 MB)
17/08/29 11:59:42 INFO MemoryStore: Block broadcast_6_piece8 stored as bytes in memory (estimated size 4.0 MB, free 109.9 MB)
17/08/29 11:59:42 INFO BlockManagerInfo: Added broadcast_6_piece8 in memory on 10.0.0.15:35768 (size: 4.0 MB, free: 310.1 MB)
17/08/29 11:59:42 INFO MemoryStore: Block broadcast_6_piece9 stored as bytes in memory (estimated size 4.0 MB, free 105.9 MB)
17/08/29 11:59:42 INFO BlockManagerInfo: Added broadcast_6_piece9 in memory on 10.0.0.15:35768 (size: 4.0 MB, free: 306.1 MB)
17/08/29 11:59:42 INFO MemoryStore: Block broadcast_6_piece10 stored as bytes in memory (estimated size 4.0 MB, free 101.9 MB)
17/08/29 11:59:42 INFO BlockManagerInfo: Added broadcast_6_piece10 in memory on 10.0.0.15:35768 (size: 4.0 MB, free: 302.1 MB)
17/08/29 11:59:42 INFO MemoryStore: Block broadcast_6_piece11 stored as bytes in memory (estimated size 4.0 MB, free 97.9 MB)
17/08/29 11:59:42 INFO BlockManagerInfo: Added broadcast_6_piece11 in memory on 10.0.0.15:35768 (size: 4.0 MB, free: 298.1 MB)
17/08/29 11:59:42 INFO MemoryStore: Block broadcast_6_piece12 stored as bytes in memory (estimated size 4.0 MB, free 93.9 MB)
17/08/29 11:59:42 INFO BlockManagerInfo: Added broadcast_6_piece12 in memory on 10.0.0.15:35768 (size: 4.0 MB, free: 294.1 MB)
17/08/29 11:59:42 INFO MemoryStore: Block broadcast_6_piece13 stored as bytes in memory (estimated size 4.0 MB, free 89.9 MB)
17/08/29 11:59:42 INFO BlockManagerInfo: Added broadcast_6_piece13 in memory on 10.0.0.15:35768 (size: 4.0 MB, free: 290.1 MB)
17/08/29 11:59:42 INFO MemoryStore: Block broadcast_6_piece14 stored as bytes in memory (estimated size 4.0 MB, free 85.9 MB)
17/08/29 11:59:42 INFO BlockManagerInfo: Added broadcast_6_piece14 in memory on 10.0.0.15:35768 (size: 4.0 MB, free: 286.1 MB)
17/08/29 11:59:42 INFO MemoryStore: Block broadcast_6_piece15 stored as bytes in memory (estimated size 4.0 MB, free 81.9 MB)
17/08/29 11:59:42 INFO BlockManagerInfo: Added broadcast_6_piece15 in memory on 10.0.0.15:35768 (size: 4.0 MB, free: 282.1 MB)
17/08/29 11:59:42 INFO MemoryStore: Block broadcast_6_piece16 stored as bytes in memory (estimated size 4.0 MB, free 77.9 MB)
17/08/29 11:59:42 INFO BlockManagerInfo: Added broadcast_6_piece16 in memory on 10.0.0.15:35768 (size: 4.0 MB, free: 278.1 MB)
17/08/29 11:59:42 INFO MemoryStore: Block broadcast_6_piece17 stored as bytes in memory (estimated size 4.0 MB, free 73.9 MB)
17/08/29 11:59:42 INFO BlockManagerInfo: Added broadcast_6_piece17 in memory on 10.0.0.15:35768 (size: 4.0 MB, free: 274.1 MB)
17/08/29 11:59:42 INFO MemoryStore: Block broadcast_6_piece18 stored as bytes in memory (estimated size 4.0 MB, free 69.9 MB)
17/08/29 11:59:42 INFO BlockManagerInfo: Added broadcast_6_piece18 in memory on 10.0.0.15:35768 (size: 4.0 MB, free: 270.1 MB)
17/08/29 11:59:42 INFO MemoryStore: Block broadcast_6_piece19 stored as bytes in memory (estimated size 3.7 MB, free 66.3 MB)
17/08/29 11:59:42 INFO BlockManagerInfo: Added broadcast_6_piece19 in memory on 10.0.0.15:35768 (size: 3.7 MB, free: 266.4 MB)
17/08/29 11:59:42 INFO SparkContext: Created broadcast 6 from broadcast at Word2Vec.scala:359
17/08/29 11:59:42 INFO MemoryStore: 5 blocks selected for dropping (98.6 MB bytes)
17/08/29 11:59:42 INFO BlockManager: Dropping block broadcast_0 from memory
17/08/29 11:59:42 INFO BlockManager: Writing block broadcast_0 to disk
17/08/29 11:59:42 INFO BlockManager: Dropping block broadcast_0_piece0 from memory
17/08/29 11:59:42 INFO BlockManager: Writing block broadcast_0_piece0 to disk
17/08/29 11:59:42 INFO BlockManagerInfo: Updated broadcast_0_piece0 on disk on 10.0.0.15:35768 (current size: 23.2 KB, original size: 0.0 B)
17/08/29 11:59:42 INFO BlockManager: Dropping block broadcast_3 from memory
17/08/29 11:59:42 INFO BlockManager: Writing block broadcast_3 to disk
17/08/29 11:59:42 INFO BlockManager: Dropping block broadcast_3_piece0 from memory
17/08/29 11:59:42 INFO BlockManager: Writing block broadcast_3_piece0 to disk
17/08/29 11:59:42 INFO BlockManagerInfo: Updated broadcast_3_piece0 on disk on 10.0.0.15:35768 (current size: 4.0 KB, original size: 0.0 B)
17/08/29 11:59:42 INFO BlockManager: Dropping block broadcast_4 from memory
17/08/29 11:59:42 INFO BlockManager: Writing block broadcast_4 to disk
17/08/29 11:59:43 INFO MemoryStore: After dropping 5 blocks, free memory is 164.9 MB
17/08/29 11:59:43 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 79.6 MB, free 85.2 MB)
17/08/29 11:59:43 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 398.1 KB, free 84.8 MB)
17/08/29 11:59:43 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.0.15:35768 (size: 398.1 KB, free: 266.0 MB)
17/08/29 11:59:43 INFO SparkContext: Created broadcast 7 from broadcast at Word2Vec.scala:360
17/08/29 11:59:43 INFO SparkContext: Starting job: collect at Word2Vec.scala:438
17/08/29 11:59:43 INFO DAGScheduler: Registering RDD 9 (repartition at Word2Vec.scala:344)
17/08/29 11:59:43 INFO DAGScheduler: Registering RDD 13 (mapPartitionsWithIndex at Word2Vec.scala:361)
17/08/29 11:59:43 INFO DAGScheduler: Got job 1 (collect at Word2Vec.scala:438) with 1 output partitions
17/08/29 11:59:43 INFO DAGScheduler: Final stage: ResultStage 4 (collect at Word2Vec.scala:438)
17/08/29 11:59:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
17/08/29 11:59:43 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
17/08/29 11:59:43 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[9] at repartition at Word2Vec.scala:344), which has no missing parents
17/08/29 11:59:43 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 6.0 KB, free 84.8 MB)
17/08/29 11:59:43 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.3 KB, free 84.8 MB)
17/08/29 11:59:43 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.0.15:35768 (size: 3.3 KB, free: 266.0 MB)
17/08/29 11:59:43 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1006
17/08/29 11:59:43 INFO DAGScheduler: Submitting 20 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[9] at repartition at Word2Vec.scala:344) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
17/08/29 11:59:43 INFO TaskSchedulerImpl: Adding task set 2.0 with 20 tasks
17/08/29 11:59:43 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 40, 10.0.0.10, executor 0, partition 0, ANY, 4877 bytes)
17/08/29 11:59:43 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 41, 10.0.0.10, executor 0, partition 1, ANY, 4877 bytes)
17/08/29 11:59:43 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.0.10:38807 (size: 3.3 KB, free: 1295.4 MB)
17/08/29 11:59:43 INFO BlockManagerInfo: Added broadcast_5_piece1 in memory on 10.0.0.10:38807 (size: 3.2 MB, free: 1292.1 MB)
17/08/29 11:59:43 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.0.10:38807 (size: 4.0 MB, free: 1288.1 MB)
17/08/29 11:59:46 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 42, 10.0.0.10, executor 0, partition 2, ANY, 4877 bytes)
17/08/29 11:59:46 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 43, 10.0.0.10, executor 0, partition 3, ANY, 4877 bytes)
17/08/29 11:59:46 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 41) in 3327 ms on 10.0.0.10 (executor 0) (1/20)
17/08/29 11:59:46 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 40) in 3328 ms on 10.0.0.10 (executor 0) (2/20)
17/08/29 11:59:49 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 44, 10.0.0.10, executor 0, partition 4, ANY, 4877 bytes)
17/08/29 11:59:49 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 42) in 2525 ms on 10.0.0.10 (executor 0) (3/20)
17/08/29 11:59:49 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 45, 10.0.0.10, executor 0, partition 5, ANY, 4877 bytes)
17/08/29 11:59:49 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 43) in 2668 ms on 10.0.0.10 (executor 0) (4/20)
17/08/29 11:59:51 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 46, 10.0.0.10, executor 0, partition 6, ANY, 4877 bytes)
17/08/29 11:59:51 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 44) in 2517 ms on 10.0.0.10 (executor 0) (5/20)
17/08/29 11:59:52 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 47, 10.0.0.10, executor 0, partition 7, ANY, 4877 bytes)
17/08/29 11:59:52 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 45) in 2515 ms on 10.0.0.10 (executor 0) (6/20)
17/08/29 11:59:54 INFO TaskSetManager: Starting task 8.0 in stage 2.0 (TID 48, 10.0.0.10, executor 0, partition 8, ANY, 4877 bytes)
17/08/29 11:59:54 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 46) in 2625 ms on 10.0.0.10 (executor 0) (7/20)
17/08/29 11:59:54 INFO TaskSetManager: Starting task 9.0 in stage 2.0 (TID 49, 10.0.0.10, executor 0, partition 9, ANY, 4877 bytes)
17/08/29 11:59:54 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 47) in 2585 ms on 10.0.0.10 (executor 0) (8/20)
17/08/29 11:59:57 INFO TaskSetManager: Starting task 10.0 in stage 2.0 (TID 50, 10.0.0.10, executor 0, partition 10, ANY, 4877 bytes)
17/08/29 11:59:57 INFO TaskSetManager: Finished task 8.0 in stage 2.0 (TID 48) in 2539 ms on 10.0.0.10 (executor 0) (9/20)
17/08/29 11:59:57 INFO TaskSetManager: Starting task 11.0 in stage 2.0 (TID 51, 10.0.0.10, executor 0, partition 11, ANY, 4877 bytes)
17/08/29 11:59:57 INFO TaskSetManager: Finished task 9.0 in stage 2.0 (TID 49) in 2647 ms on 10.0.0.10 (executor 0) (10/20)
17/08/29 11:59:59 INFO TaskSetManager: Starting task 12.0 in stage 2.0 (TID 52, 10.0.0.10, executor 0, partition 12, ANY, 4877 bytes)
17/08/29 11:59:59 INFO TaskSetManager: Finished task 10.0 in stage 2.0 (TID 50) in 2692 ms on 10.0.0.10 (executor 0) (11/20)
17/08/29 11:59:59 INFO TaskSetManager: Starting task 13.0 in stage 2.0 (TID 53, 10.0.0.10, executor 0, partition 13, ANY, 4877 bytes)
17/08/29 11:59:59 INFO TaskSetManager: Finished task 11.0 in stage 2.0 (TID 51) in 2668 ms on 10.0.0.10 (executor 0) (12/20)
17/08/29 12:00:02 INFO TaskSetManager: Starting task 14.0 in stage 2.0 (TID 54, 10.0.0.10, executor 0, partition 14, ANY, 4877 bytes)
17/08/29 12:00:02 INFO TaskSetManager: Finished task 12.0 in stage 2.0 (TID 52) in 2706 ms on 10.0.0.10 (executor 0) (13/20)
17/08/29 12:00:02 INFO TaskSetManager: Starting task 15.0 in stage 2.0 (TID 55, 10.0.0.10, executor 0, partition 15, ANY, 4877 bytes)
17/08/29 12:00:02 INFO TaskSetManager: Finished task 13.0 in stage 2.0 (TID 53) in 2564 ms on 10.0.0.10 (executor 0) (14/20)
17/08/29 12:00:05 INFO TaskSetManager: Starting task 16.0 in stage 2.0 (TID 56, 10.0.0.10, executor 0, partition 16, ANY, 4877 bytes)
17/08/29 12:00:05 INFO TaskSetManager: Finished task 14.0 in stage 2.0 (TID 54) in 2561 ms on 10.0.0.10 (executor 0) (15/20)
17/08/29 12:00:05 INFO TaskSetManager: Starting task 17.0 in stage 2.0 (TID 57, 10.0.0.10, executor 0, partition 17, ANY, 4877 bytes)
17/08/29 12:00:05 INFO TaskSetManager: Finished task 15.0 in stage 2.0 (TID 55) in 2674 ms on 10.0.0.10 (executor 0) (16/20)
17/08/29 12:00:07 INFO TaskSetManager: Starting task 18.0 in stage 2.0 (TID 58, 10.0.0.10, executor 0, partition 18, ANY, 4877 bytes)
17/08/29 12:00:07 INFO TaskSetManager: Finished task 16.0 in stage 2.0 (TID 56) in 2499 ms on 10.0.0.10 (executor 0) (17/20)
17/08/29 12:00:07 INFO TaskSetManager: Starting task 19.0 in stage 2.0 (TID 59, 10.0.0.10, executor 0, partition 19, ANY, 4877 bytes)
17/08/29 12:00:07 INFO TaskSetManager: Finished task 17.0 in stage 2.0 (TID 57) in 2400 ms on 10.0.0.10 (executor 0) (18/20)
17/08/29 12:00:07 INFO TaskSetManager: Finished task 19.0 in stage 2.0 (TID 59) in 326 ms on 10.0.0.10 (executor 0) (19/20)
17/08/29 12:00:09 INFO TaskSetManager: Finished task 18.0 in stage 2.0 (TID 58) in 2295 ms on 10.0.0.10 (executor 0) (20/20)
17/08/29 12:00:09 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/08/29 12:00:09 INFO DAGScheduler: ShuffleMapStage 2 (repartition at Word2Vec.scala:344) finished in 26.275 s
17/08/29 12:00:09 INFO DAGScheduler: looking for newly runnable stages
17/08/29 12:00:09 INFO DAGScheduler: running: Set()
17/08/29 12:00:09 INFO DAGScheduler: waiting: Set(ShuffleMapStage 3, ResultStage 4)
17/08/29 12:00:09 INFO DAGScheduler: failed: Set()
17/08/29 12:00:09 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[13] at mapPartitionsWithIndex at Word2Vec.scala:361), which has no missing parents
17/08/29 12:00:09 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 6.4 KB, free 84.8 MB)
17/08/29 12:00:09 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.5 KB, free 84.8 MB)
17/08/29 12:00:09 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.0.15:35768 (size: 3.5 KB, free: 266.0 MB)
17/08/29 12:00:09 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1006
17/08/29 12:00:09 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[13] at mapPartitionsWithIndex at Word2Vec.scala:361) (first 15 tasks are for partitions Vector(0))
17/08/29 12:00:09 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
17/08/29 12:00:09 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 60, 10.0.0.10, executor 0, partition 0, NODE_LOCAL, 4890 bytes)
17/08/29 12:00:09 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.0.10:38807 (size: 3.5 KB, free: 1288.1 MB)
17/08/29 12:00:09 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.0.10:53254
17/08/29 12:00:09 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 165 bytes
17/08/29 12:00:12 INFO BlockManagerInfo: Added rdd_12_0 in memory on 10.0.0.10:38807 (size: 252.5 MB, free: 1035.7 MB)
17/08/29 12:00:12 INFO BlockManagerInfo: Added broadcast_6_piece5 in memory on 10.0.0.10:38807 (size: 4.0 MB, free: 1031.7 MB)
17/08/29 12:00:12 INFO BlockManagerInfo: Added broadcast_6_piece7 in memory on 10.0.0.10:38807 (size: 4.0 MB, free: 1027.7 MB)
17/08/29 12:00:12 INFO BlockManagerInfo: Added broadcast_6_piece9 in memory on 10.0.0.10:38807 (size: 4.0 MB, free: 1023.7 MB)
17/08/29 12:00:12 INFO BlockManagerInfo: Added broadcast_6_piece4 in memory on 10.0.0.10:38807 (size: 4.0 MB, free: 1019.7 MB)
17/08/29 12:00:12 INFO BlockManagerInfo: Added broadcast_6_piece18 in memory on 10.0.0.10:38807 (size: 4.0 MB, free: 1015.7 MB)
17/08/29 12:00:12 INFO BlockManagerInfo: Added broadcast_6_piece13 in memory on 10.0.0.10:38807 (size: 4.0 MB, free: 1011.7 MB)
17/08/29 12:00:12 INFO BlockManagerInfo: Added broadcast_6_piece10 in memory on 10.0.0.10:38807 (size: 4.0 MB, free: 1007.7 MB)
17/08/29 12:00:12 INFO BlockManagerInfo: Added broadcast_6_piece16 in memory on 10.0.0.10:38807 (size: 4.0 MB, free: 1003.7 MB)
17/08/29 12:00:12 INFO BlockManagerInfo: Added broadcast_6_piece1 in memory on 10.0.0.10:38807 (size: 4.0 MB, free: 999.7 MB)
17/08/29 12:00:12 INFO BlockManagerInfo: Added broadcast_6_piece11 in memory on 10.0.0.10:38807 (size: 4.0 MB, free: 995.7 MB)
17/08/29 12:00:12 INFO BlockManagerInfo: Added broadcast_6_piece2 in memory on 10.0.0.10:38807 (size: 4.0 MB, free: 991.7 MB)
17/08/29 12:00:12 INFO BlockManagerInfo: Added broadcast_6_piece3 in memory on 10.0.0.10:38807 (size: 4.0 MB, free: 987.7 MB)
17/08/29 12:00:12 INFO BlockManagerInfo: Added broadcast_6_piece17 in memory on 10.0.0.10:38807 (size: 4.0 MB, free: 983.7 MB)
17/08/29 12:00:12 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.0.10:38807 (size: 4.0 MB, free: 979.7 MB)
17/08/29 12:00:12 INFO BlockManagerInfo: Added broadcast_6_piece6 in memory on 10.0.0.10:38807 (size: 4.0 MB, free: 975.7 MB)
17/08/29 12:00:12 INFO BlockManagerInfo: Added broadcast_6_piece14 in memory on 10.0.0.10:38807 (size: 4.0 MB, free: 971.7 MB)
17/08/29 12:00:12 INFO BlockManagerInfo: Added broadcast_6_piece19 in memory on 10.0.0.10:38807 (size: 3.7 MB, free: 968.0 MB)
17/08/29 12:00:12 INFO BlockManagerInfo: Added broadcast_6_piece8 in memory on 10.0.0.10:38807 (size: 4.0 MB, free: 964.0 MB)
17/08/29 12:00:12 INFO BlockManagerInfo: Added broadcast_6_piece15 in memory on 10.0.0.10:38807 (size: 4.0 MB, free: 960.0 MB)
17/08/29 12:00:12 INFO BlockManagerInfo: Added broadcast_6_piece12 in memory on 10.0.0.10:38807 (size: 4.0 MB, free: 956.0 MB)
17/08/29 12:00:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.0.10:38807 (size: 398.1 KB, free: 955.6 MB)
17/08/29 12:00:14 INFO BlockManagerInfo: Added broadcast_4_piece3 in memory on 10.0.0.10:38807 (size: 1002.6 KB, free: 954.6 MB)
17/08/29 12:00:14 INFO BlockManagerInfo: Added broadcast_4_piece1 in memory on 10.0.0.10:38807 (size: 4.0 MB, free: 950.6 MB)
17/08/29 12:00:14 INFO BlockManagerInfo: Added broadcast_4_piece2 in memory on 10.0.0.10:38807 (size: 4.0 MB, free: 946.6 MB)
17/08/29 12:00:14 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.0.10:38807 (size: 4.0 MB, free: 942.6 MB)
17/08/29 12:00:15 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.0 KB, free 84.8 MB)
17/08/29 12:00:15 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.0.10:38807 (size: 4.0 KB, free: 942.6 MB)
17/08/29 12:00:35 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.0.15:35768 in memory (size: 3.3 KB, free: 266.0 MB)
17/08/29 12:00:35 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.0.10:38807 in memory (size: 3.3 KB, free: 942.6 MB)
17/08/29 12:07:07 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 60) in 417452 ms on 10.0.0.10 (executor 0) (1/1)
17/08/29 12:07:07 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/08/29 12:07:07 INFO DAGScheduler: ShuffleMapStage 3 (mapPartitionsWithIndex at Word2Vec.scala:361) finished in 417.452 s
17/08/29 12:07:07 INFO DAGScheduler: looking for newly runnable stages
17/08/29 12:07:07 INFO DAGScheduler: running: Set()
17/08/29 12:07:07 INFO DAGScheduler: waiting: Set(ResultStage 4)
17/08/29 12:07:07 INFO DAGScheduler: failed: Set()
17/08/29 12:07:07 INFO DAGScheduler: Submitting ResultStage 4 (ShuffledRDD[14] at reduceByKey at Word2Vec.scala:435), which has no missing parents
17/08/29 12:07:07 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 4.4 KB, free 84.8 MB)
17/08/29 12:07:07 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.4 KB, free 84.8 MB)
17/08/29 12:07:07 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.0.15:35768 (size: 2.4 KB, free: 266.0 MB)
17/08/29 12:07:07 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1006
17/08/29 12:07:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (ShuffledRDD[14] at reduceByKey at Word2Vec.scala:435) (first 15 tasks are for partitions Vector(0))
17/08/29 12:07:07 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
17/08/29 12:07:07 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 61, 10.0.0.10, executor 0, partition 0, NODE_LOCAL, 4625 bytes)
17/08/29 12:07:07 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.0.10:38807 (size: 2.4 KB, free: 942.6 MB)
17/08/29 12:07:07 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.0.10:53254
17/08/29 12:07:07 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 137 bytes
17/08/29 12:07:10 INFO BlockManagerInfo: Added taskresult_61 in memory on 10.0.0.10:38807 (size: 169.8 MB, free: 772.8 MB)
17/08/29 12:07:19 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.0.15:35768 in memory (size: 3.5 KB, free: 266.0 MB)
17/08/29 12:07:19 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.0.10:38807 in memory (size: 3.5 KB, free: 772.8 MB)
17/08/29 12:18:36 WARN TransportChannelHandler: Exception in connection from /10.0.0.10:53254
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:221)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:899)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:275)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:748)
17/08/29 12:19:46 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=5330921658726773156, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]}} to /10.0.0.10:53254; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 12:19:58 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=7072127789328119647, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]}} to /10.0.0.10:53254; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 12:20:08 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=8796221580576455061, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]}} to /10.0.0.10:53254; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 12:20:08 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=6344071470805691253, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]}} to /10.0.0.10:53254; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 12:20:09 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=5389114041839999220, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]}} to /10.0.0.10:53254; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 12:20:29 WARN HeartbeatReceiver: Removing executor 0 with no recent heartbeats: 132572 ms exceeds timeout 120000 ms
17/08/29 12:20:38 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=6844435563681403787, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]}} to /10.0.0.10:53254; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 12:20:44 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=6414468225513165565, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]}} to /10.0.0.10:53254; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 12:21:09 ERROR TaskSchedulerImpl: Lost executor 0 on 10.0.0.10: Executor heartbeat timed out after 132572 ms
17/08/29 12:21:15 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=6823645276084025602, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]}} to /10.0.0.10:53254; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 12:21:23 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=8057141131232730191, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]}} to /10.0.0.10:53254; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 12:21:25 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=8618039349184128285, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]}} to /10.0.0.10:53254; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 12:22:59 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 61, 10.0.0.10, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 132572 ms
17/08/29 12:24:21 INFO DAGScheduler: Executor lost: 0 (epoch 3)
17/08/29 12:24:25 INFO StandaloneSchedulerBackend: Requesting to kill executor(s) 0
17/08/29 12:24:46 WARN NettyRpcEnv: Ignored message: true
17/08/29 12:24:52 INFO BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
17/08/29 12:25:20 WARN BlockManagerMasterEndpoint: No more replicas available for taskresult_61 !
17/08/29 12:25:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_12_0 !
17/08/29 12:26:01 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(0, 10.0.0.10, 38807, None)
17/08/29 12:26:05 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=8289752155928086038, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]}} to /10.0.0.10:53254; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 12:26:10 INFO BlockManagerMaster: Removed 0 successfully in removeExecutor
17/08/29 12:26:16 INFO DAGScheduler: Shuffle files lost for executor: 0 (epoch 3)
17/08/29 12:26:32 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/0 is now EXITED (Command exited with code 56)
17/08/29 12:26:35 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=8848793849610248499, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=47 cap=64]}} to /10.0.0.10:53348; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 12:26:36 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=8074023361149063614, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=47 cap=64]}} to /10.0.0.10:53346; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 12:26:38 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=8246780345893481192, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=47 cap=64]}} to /10.0.0.10:53352; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 12:26:40 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=7497518224413144210, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=47 cap=64]}} to /10.0.0.10:53350; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 12:26:51 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/0 removed: Command exited with code 56
17/08/29 12:27:35 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/1 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 12:27:51 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/1 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 12:27:53 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/1 is now RUNNING
17/08/29 12:28:04 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/1 is now EXITED (Command exited with code 1)
17/08/29 12:28:04 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/1 removed: Command exited with code 1
17/08/29 12:28:12 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/2 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 12:28:20 INFO ShuffleMapStage: ShuffleMapStage 3 is now unavailable on executor 0 (0/1, false)
17/08/29 12:28:32 INFO ShuffleMapStage: ShuffleMapStage 2 is now unavailable on executor 0 (0/20, false)
17/08/29 12:28:35 INFO DAGScheduler: Host added was in lost list earlier: 10.0.0.10
17/08/29 12:28:35 INFO TaskSetManager: Starting task 0.1 in stage 4.0 (TID 62, 10.0.0.10, executor 0, partition 0, NODE_LOCAL, 4625 bytes)
17/08/29 12:28:52 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/2 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 12:29:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/2 is now RUNNING
17/08/29 12:29:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/2 is now EXITED (Command exited with code 1)
17/08/29 12:29:10 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/2 removed: Command exited with code 1
17/08/29 12:29:14 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/3 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 12:29:16 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/3 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 12:29:16 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/3 is now RUNNING
17/08/29 12:29:19 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/3 is now EXITED (Command exited with code 1)
17/08/29 12:29:20 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/3 removed: Command exited with code 1
17/08/29 12:29:20 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/4 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 12:29:23 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/4 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 12:29:23 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/4 is now RUNNING
17/08/29 12:29:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/4 is now EXITED (Command exited with code 1)
17/08/29 12:29:24 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/4 removed: Command exited with code 1
17/08/29 12:29:25 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/5 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 12:29:34 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/5 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 12:29:34 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/5 is now RUNNING
17/08/29 12:29:34 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/5 is now EXITED (Command exited with code 1)
17/08/29 12:29:34 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/5 removed: Command exited with code 1
17/08/29 12:29:35 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/6 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 12:29:35 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/6 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 12:29:36 INFO StandaloneSchedulerBackend: Actual list of executor(s) to be killed is 0
17/08/29 12:29:38 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/6 is now RUNNING
17/08/29 12:29:49 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 12:30:36 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 12:30:46 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=8666767681490640242, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=47 cap=64]}} to /10.0.0.10:53356; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 12:30:37 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=5279133673330841402, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=47 cap=64]}} to /10.0.0.10:53354; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 12:31:13 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 12:31:23 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=7999406643179689572, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]}} to /10.0.0.10:53254; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 12:31:37 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 12:32:02 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 12:32:30 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 12:32:54 ERROR Utils: Uncaught exception in thread kill-executor-thread
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.killExecutors(CoarseGrainedSchedulerBackend.scala:637)
	at org.apache.spark.SparkContext.killAndReplaceExecutor(SparkContext.scala:1687)
	at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3$$anon$3$$anonfun$run$3.apply$mcV$sp(HeartbeatReceiver.scala:207)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317)
	at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3$$anon$3.run(HeartbeatReceiver.scala:204)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 10 more
17/08/29 12:34:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/6 is now EXITED (Command exited with code 1)
17/08/29 12:34:24 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/6 removed: Command exited with code 1
17/08/29 12:34:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/7 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 12:34:27 WARN NettyRpcEnv: Ignored message: true
17/08/29 12:34:34 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/7 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 12:35:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/7 is now RUNNING
17/08/29 12:35:02 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/7 is now EXITED (Command exited with code 1)
17/08/29 12:35:03 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/7 removed: Command exited with code 1
17/08/29 12:35:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/8 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 12:35:15 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/8 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 12:35:18 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/8 is now RUNNING
17/08/29 12:35:19 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/8 is now EXITED (Command exited with code 1)
17/08/29 12:35:19 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/8 removed: Command exited with code 1
17/08/29 12:35:28 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/9 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 12:35:32 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/9 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 12:35:33 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/9 is now RUNNING
17/08/29 12:35:51 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=8413884360036781833, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=47 cap=64]}} to /10.0.0.10:53358; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 12:35:51 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=5181150330938399719, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=47 cap=64]}} to /10.0.0.10:53360; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 12:36:31 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 12:37:09 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 12:37:18 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=9059788271005153756, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]}} to /10.0.0.10:53254; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 12:37:28 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 12:40:57 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/9 is now EXITED (Command exited with code 1)
17/08/29 12:41:17 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/9 removed: Command exited with code 1
17/08/29 12:41:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/10 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 12:41:31 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/10 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 12:41:32 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/10 is now RUNNING
17/08/29 12:42:35 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/10 is now EXITED (Command exited with code 1)
17/08/29 12:42:35 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/10 removed: Command exited with code 1
17/08/29 12:42:41 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/11 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 12:43:13 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/11 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 12:43:17 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/11 is now RUNNING
17/08/29 12:43:33 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 12:44:02 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/11 is now EXITED (Command exited with code 1)
17/08/29 12:44:04 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/11 removed: Command exited with code 1
17/08/29 12:44:15 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/12 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 12:44:32 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/12 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 12:44:41 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/12 is now RUNNING
17/08/29 12:44:50 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 12:46:07 ERROR TaskSchedulerImpl: Lost executor 0 on 10.0.0.10: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/08/29 12:46:07 WARN TaskSetManager: Lost task 0.1 in stage 4.0 (TID 62, 10.0.0.10, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/08/29 12:46:13 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 12:46:21 INFO DAGScheduler: Executor lost: 0 (epoch 5)
17/08/29 12:46:44 INFO BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
17/08/29 12:46:45 INFO BlockManagerMaster: Removed 0 successfully in removeExecutor
17/08/29 12:46:53 INFO DAGScheduler: Shuffle files lost for executor: 0 (epoch 5)
17/08/29 12:50:44 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/12 is now EXITED (Command exited with code 1)
17/08/29 12:51:04 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/12 removed: Command exited with code 1
17/08/29 12:51:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/13 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 12:52:10 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/13 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 12:53:09 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 12:55:16 INFO BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
17/08/29 12:55:29 INFO BlockManagerMaster: Removal of executor 0 requested
17/08/29 12:55:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 0
17/08/29 12:55:52 WARN NettyRpcEnv: Ignored message: true
17/08/29 12:56:09 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/13 is now RUNNING
17/08/29 12:57:53 INFO BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
17/08/29 12:57:54 INFO BlockManagerMaster: Removal of executor 1 requested
17/08/29 12:57:57 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 1
17/08/29 12:58:00 WARN NettyRpcEnv: Ignored message: true
17/08/29 12:58:20 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/13 is now EXITED (Command exited with code 1)
17/08/29 12:58:20 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/13 removed: Command exited with code 1
17/08/29 12:58:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/14 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 12:58:56 INFO BlockManagerMaster: Removal of executor 2 requested
17/08/29 12:58:57 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 2
17/08/29 12:59:00 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/14 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 12:59:04 INFO BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.
17/08/29 12:59:03 WARN NettyRpcEnv: Ignored message: true
17/08/29 12:59:09 INFO BlockManagerMasterEndpoint: Trying to remove executor 3 from BlockManagerMaster.
17/08/29 12:59:17 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/14 is now RUNNING
17/08/29 12:59:19 INFO BlockManagerMaster: Removal of executor 3 requested
17/08/29 12:59:21 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 3
17/08/29 12:59:25 WARN NettyRpcEnv: Ignored message: true
17/08/29 12:59:28 INFO BlockManagerMaster: Removal of executor 4 requested
17/08/29 12:59:30 INFO BlockManagerMasterEndpoint: Trying to remove executor 4 from BlockManagerMaster.
17/08/29 12:59:31 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 4
17/08/29 12:59:38 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/14 is now EXITED (Command exited with code 1)
17/08/29 12:59:38 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/14 removed: Command exited with code 1
17/08/29 12:59:39 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/15 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 12:59:42 WARN NettyRpcEnv: Ignored message: true
17/08/29 12:59:57 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/15 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 13:00:04 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/15 is now RUNNING
17/08/29 13:00:06 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/15 is now EXITED (Command exited with code 1)
17/08/29 13:00:13 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/15 removed: Command exited with code 1
17/08/29 13:00:16 INFO BlockManagerMaster: Removal of executor 5 requested
17/08/29 13:00:24 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 5
17/08/29 13:00:23 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 13:00:26 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/16 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 13:00:24 WARN NettyRpcEnv: Ignored message: true
17/08/29 13:01:11 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/16 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 13:01:20 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/16 is now RUNNING
17/08/29 13:01:34 INFO BlockManagerMasterEndpoint: Trying to remove executor 5 from BlockManagerMaster.
17/08/29 13:01:43 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 13:01:48 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/16 is now EXITED (Command exited with code 1)
17/08/29 13:02:21 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/16 removed: Command exited with code 1
17/08/29 13:02:28 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/17 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 13:02:37 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 13:02:40 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/17 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 13:02:54 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/17 is now RUNNING
17/08/29 13:02:59 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/17 is now EXITED (Command exited with code 1)
17/08/29 13:03:01 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/17 removed: Command exited with code 1
17/08/29 13:03:10 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/18 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 13:03:16 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/18 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 13:03:17 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/18 is now RUNNING
17/08/29 13:04:19 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/18 is now EXITED (Command exited with code 1)
17/08/29 13:04:21 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/18 removed: Command exited with code 1
17/08/29 13:04:25 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/19 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 13:04:31 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/19 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 13:04:32 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 13:04:34 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/19 is now RUNNING
17/08/29 13:05:07 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 13:06:35 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 13:09:20 INFO BlockManagerMaster: Removal of executor 6 requested
17/08/29 13:09:20 INFO BlockManagerMasterEndpoint: Trying to remove executor 6 from BlockManagerMaster.
17/08/29 13:09:22 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 6
17/08/29 13:09:33 WARN NettyRpcEnv: Ignored message: true
17/08/29 13:10:35 INFO BlockManagerMaster: Removal of executor 7 requested
17/08/29 13:10:36 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 7
17/08/29 13:10:38 WARN NettyRpcEnv: Ignored message: true
17/08/29 13:10:45 INFO BlockManagerMasterEndpoint: Trying to remove executor 7 from BlockManagerMaster.
17/08/29 13:10:52 INFO BlockManagerMaster: Removal of executor 8 requested
17/08/29 13:10:57 INFO BlockManagerMasterEndpoint: Trying to remove executor 8 from BlockManagerMaster.
17/08/29 13:11:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 8
17/08/29 13:11:04 WARN NettyRpcEnv: Ignored message: true
17/08/29 13:12:23 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/19 is now EXITED (Command exited with code 1)
17/08/29 13:12:24 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/19 removed: Command exited with code 1
17/08/29 13:13:08 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/20 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 13:13:13 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/20 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 13:14:26 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 13:14:26 ERROR Utils: Uncaught exception in thread task-result-getter-0
java.lang.OutOfMemoryError: Java heap space
	at java.lang.reflect.Array.newInstance(Array.java:75)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1897)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:88)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:94)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:62)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/08/29 13:14:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/20 is now RUNNING
17/08/29 13:14:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/20 is now EXITED (Command exited with code 1)
17/08/29 13:14:26 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/20 removed: Command exited with code 1
17/08/29 13:14:26 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=7240075277511356579, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53362; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 13:14:26 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/21 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 13:14:26 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/21 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 13:14:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/21 is now RUNNING
17/08/29 13:14:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/21 is now EXITED (Command exited with code 1)
17/08/29 13:14:26 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/21 removed: Command exited with code 1
17/08/29 13:14:26 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/22 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 13:14:26 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/22 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 13:14:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/22 is now RUNNING
Exception in thread "task-result-getter-0" java.lang.OutOfMemoryError: Java heap space
	at java.lang.reflect.Array.newInstance(Array.java:75)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1897)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:88)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:94)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:62)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/08/29 13:14:26 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=5737890226177912556, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53364; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 13:14:26 INFO BlockManagerMasterEndpoint: Trying to remove executor 9 from BlockManagerMaster.
17/08/29 13:14:26 INFO BlockManagerMaster: Removal of executor 9 requested
17/08/29 13:14:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 9
17/08/29 13:14:26 WARN NettyRpcEnv: Ignored message: true
17/08/29 13:14:26 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=7621883270683145217, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53366; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 13:14:26 INFO BlockManagerMasterEndpoint: Trying to remove executor 10 from BlockManagerMaster.
17/08/29 13:14:26 INFO BlockManagerMaster: Removal of executor 10 requested
17/08/29 13:14:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 10
17/08/29 13:14:26 WARN NettyRpcEnv: Ignored message: true
17/08/29 13:14:26 INFO BlockManagerMasterEndpoint: Trying to remove executor 11 from BlockManagerMaster.
17/08/29 13:14:26 INFO BlockManagerMaster: Removal of executor 11 requested
17/08/29 13:14:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 11
17/08/29 13:14:26 WARN NettyRpcEnv: Ignored message: true
17/08/29 13:14:26 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=7279891600694070445, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53368; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 13:14:26 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=6387292109048532348, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53370; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 13:14:26 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=7354278535636548437, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53372; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 13:14:26 INFO BlockManagerMasterEndpoint: Trying to remove executor 12 from BlockManagerMaster.
17/08/29 13:14:26 INFO BlockManagerMaster: Removal of executor 12 requested
17/08/29 13:14:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 12
17/08/29 13:14:26 WARN NettyRpcEnv: Ignored message: true
17/08/29 13:14:26 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=4923236333971823615, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53374; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 13:14:26 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=6289667685567160535, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53376; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 13:14:26 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=8623194661790485382, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53378; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 13:14:26 INFO BlockManagerMasterEndpoint: Trying to remove executor 13 from BlockManagerMaster.
17/08/29 13:14:26 INFO BlockManagerMaster: Removal of executor 13 requested
17/08/29 13:14:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 13
17/08/29 13:14:26 WARN NettyRpcEnv: Ignored message: true
17/08/29 13:14:26 INFO BlockManagerMasterEndpoint: Trying to remove executor 14 from BlockManagerMaster.
17/08/29 13:14:26 INFO BlockManagerMaster: Removal of executor 14 requested
17/08/29 13:14:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 14
17/08/29 13:14:26 WARN NettyRpcEnv: Ignored message: true
17/08/29 13:14:26 INFO BlockManagerMasterEndpoint: Trying to remove executor 15 from BlockManagerMaster.
17/08/29 13:14:26 INFO BlockManagerMaster: Removal of executor 15 requested
17/08/29 13:14:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 15
17/08/29 13:14:26 WARN NettyRpcEnv: Ignored message: true
17/08/29 13:14:26 INFO BlockManagerMasterEndpoint: Trying to remove executor 16 from BlockManagerMaster.
17/08/29 13:14:26 INFO BlockManagerMaster: Removal of executor 16 requested
17/08/29 13:14:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 16
17/08/29 13:14:26 WARN NettyRpcEnv: Ignored message: true
17/08/29 13:14:26 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=6900971077918552715, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53380; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 13:14:26 INFO BlockManagerMasterEndpoint: Trying to remove executor 17 from BlockManagerMaster.
17/08/29 13:14:26 INFO BlockManagerMaster: Removal of executor 17 requested
17/08/29 13:14:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 17
17/08/29 13:14:26 WARN NettyRpcEnv: Ignored message: true
17/08/29 13:14:26 INFO BlockManagerMasterEndpoint: Trying to remove executor 18 from BlockManagerMaster.
17/08/29 13:14:26 INFO BlockManagerMaster: Removal of executor 18 requested
17/08/29 13:14:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 18
17/08/29 13:14:26 WARN NettyRpcEnv: Ignored message: true
17/08/29 13:14:26 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=6681550975301693701, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53382; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 13:14:26 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=4878885156517175221, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53384; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 13:14:26 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=8291997267067483176, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53386; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 13:14:26 INFO BlockManagerMasterEndpoint: Trying to remove executor 19 from BlockManagerMaster.
17/08/29 13:14:26 INFO BlockManagerMaster: Removal of executor 19 requested
17/08/29 13:14:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 19
17/08/29 13:14:26 WARN NettyRpcEnv: Ignored message: true
17/08/29 13:14:26 INFO BlockManagerMasterEndpoint: Trying to remove executor 20 from BlockManagerMaster.
17/08/29 13:14:26 INFO BlockManagerMaster: Removal of executor 20 requested
17/08/29 13:14:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 20
17/08/29 13:14:26 INFO BlockManagerMasterEndpoint: Trying to remove executor 21 from BlockManagerMaster.
17/08/29 13:14:26 INFO BlockManagerMaster: Removal of executor 21 requested
17/08/29 13:14:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 21
17/08/29 13:14:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.0.10:53390) with ID 22
17/08/29 13:14:26 INFO TaskSetManager: Starting task 0.2 in stage 4.0 (TID 63, 10.0.0.10, executor 22, partition 0, NODE_LOCAL, 4625 bytes)
17/08/29 13:14:26 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.0.10:46456 with 1295.4 MB RAM, BlockManagerId(22, 10.0.0.10, 46456, None)
17/08/29 13:14:27 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.0.10:46456 (size: 2.4 KB, free: 1295.4 MB)
17/08/29 13:14:27 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.0.10:53390
17/08/29 13:14:27 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 85 bytes
17/08/29 13:14:27 WARN TaskSetManager: Lost task 0.2 in stage 4.0 (TID 63, 10.0.0.10, executor 22): FetchFailed(null, shuffleId=2, mapId=-1, reduceId=0, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 2
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

)
17/08/29 13:14:27 INFO TaskSetManager: Task 0.2 in stage 4.0 (TID 63) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
17/08/29 13:14:27 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
17/08/29 13:14:27 INFO DAGScheduler: Marking ResultStage 4 (collect at Word2Vec.scala:438) as failed due to a fetch failure from ShuffleMapStage 3 (mapPartitionsWithIndex at Word2Vec.scala:361)
17/08/29 13:14:27 INFO DAGScheduler: ResultStage 4 (collect at Word2Vec.scala:438) failed in 4040.526 s due to org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 2
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

17/08/29 13:14:27 INFO DAGScheduler: Resubmitting ShuffleMapStage 3 (mapPartitionsWithIndex at Word2Vec.scala:361) and ResultStage 4 (collect at Word2Vec.scala:438) due to fetch failure
17/08/29 13:14:28 INFO DAGScheduler: Resubmitting failed stages
17/08/29 13:14:28 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[9] at repartition at Word2Vec.scala:344), which has no missing parents
17/08/29 13:14:28 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 6.0 KB, free 84.8 MB)
17/08/29 13:14:28 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 3.3 KB, free 84.8 MB)
17/08/29 13:14:28 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.0.15:35768 (size: 3.3 KB, free: 266.0 MB)
17/08/29 13:14:28 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1006
17/08/29 13:14:28 INFO DAGScheduler: Submitting 20 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[9] at repartition at Word2Vec.scala:344) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
17/08/29 13:14:28 INFO TaskSchedulerImpl: Adding task set 2.1 with 20 tasks
17/08/29 13:14:28 INFO TaskSetManager: Starting task 0.0 in stage 2.1 (TID 64, 10.0.0.10, executor 22, partition 0, ANY, 4877 bytes)
17/08/29 13:14:28 INFO TaskSetManager: Starting task 1.0 in stage 2.1 (TID 65, 10.0.0.10, executor 22, partition 1, ANY, 4877 bytes)
17/08/29 13:14:28 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.0.10:46456 (size: 3.3 KB, free: 1295.4 MB)
17/08/29 13:14:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.2 KB, free 84.8 MB)
17/08/29 13:14:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.0.10:46456 (size: 23.2 KB, free: 1295.4 MB)
17/08/29 13:14:30 INFO BlockManagerInfo: Added broadcast_5_piece1 in memory on 10.0.0.10:46456 (size: 3.2 MB, free: 1292.1 MB)
17/08/29 13:14:30 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.0.10:46456 (size: 4.0 MB, free: 1288.1 MB)
17/08/29 13:14:33 INFO TaskSetManager: Starting task 2.0 in stage 2.1 (TID 66, 10.0.0.10, executor 22, partition 2, ANY, 4877 bytes)
17/08/29 13:14:33 INFO TaskSetManager: Finished task 1.0 in stage 2.1 (TID 65) in 5772 ms on 10.0.0.10 (executor 22) (1/20)
17/08/29 13:14:34 INFO TaskSetManager: Finished task 0.0 in stage 2.1 (TID 64) in 6225 ms on 10.0.0.10 (executor 22) (2/20)
17/08/29 13:14:34 INFO TaskSetManager: Starting task 3.0 in stage 2.1 (TID 67, 10.0.0.10, executor 22, partition 3, ANY, 4877 bytes)
17/08/29 13:14:36 INFO TaskSetManager: Starting task 4.0 in stage 2.1 (TID 68, 10.0.0.10, executor 22, partition 4, ANY, 4877 bytes)
17/08/29 13:14:36 INFO TaskSetManager: Finished task 2.0 in stage 2.1 (TID 66) in 2403 ms on 10.0.0.10 (executor 22) (3/20)
17/08/29 13:14:36 INFO TaskSetManager: Finished task 3.0 in stage 2.1 (TID 67) in 2379 ms on 10.0.0.10 (executor 22) (4/20)
17/08/29 13:14:36 INFO TaskSetManager: Starting task 5.0 in stage 2.1 (TID 69, 10.0.0.10, executor 22, partition 5, ANY, 4877 bytes)
17/08/29 13:14:38 INFO TaskSetManager: Starting task 6.0 in stage 2.1 (TID 70, 10.0.0.10, executor 22, partition 6, ANY, 4877 bytes)
17/08/29 13:14:38 INFO TaskSetManager: Finished task 4.0 in stage 2.1 (TID 68) in 2392 ms on 10.0.0.10 (executor 22) (5/20)
17/08/29 13:14:39 INFO TaskSetManager: Starting task 7.0 in stage 2.1 (TID 71, 10.0.0.10, executor 22, partition 7, ANY, 4877 bytes)
17/08/29 13:14:39 INFO TaskSetManager: Finished task 5.0 in stage 2.1 (TID 69) in 2342 ms on 10.0.0.10 (executor 22) (6/20)
17/08/29 13:14:41 INFO TaskSetManager: Starting task 8.0 in stage 2.1 (TID 72, 10.0.0.10, executor 22, partition 8, ANY, 4877 bytes)
17/08/29 13:14:41 INFO TaskSetManager: Finished task 6.0 in stage 2.1 (TID 70) in 2584 ms on 10.0.0.10 (executor 22) (7/20)
17/08/29 13:14:41 INFO TaskSetManager: Starting task 9.0 in stage 2.1 (TID 73, 10.0.0.10, executor 22, partition 9, ANY, 4877 bytes)
17/08/29 13:14:41 INFO TaskSetManager: Finished task 7.0 in stage 2.1 (TID 71) in 2509 ms on 10.0.0.10 (executor 22) (8/20)
17/08/29 13:14:43 INFO TaskSetManager: Starting task 10.0 in stage 2.1 (TID 74, 10.0.0.10, executor 22, partition 10, ANY, 4877 bytes)
17/08/29 13:14:43 INFO TaskSetManager: Finished task 8.0 in stage 2.1 (TID 72) in 2325 ms on 10.0.0.10 (executor 22) (9/20)
17/08/29 13:14:44 INFO TaskSetManager: Starting task 11.0 in stage 2.1 (TID 75, 10.0.0.10, executor 22, partition 11, ANY, 4877 bytes)
17/08/29 13:14:44 INFO TaskSetManager: Finished task 9.0 in stage 2.1 (TID 73) in 2548 ms on 10.0.0.10 (executor 22) (10/20)
17/08/29 13:14:45 INFO TaskSetManager: Starting task 12.0 in stage 2.1 (TID 76, 10.0.0.10, executor 22, partition 12, ANY, 4877 bytes)
17/08/29 13:14:45 INFO TaskSetManager: Finished task 10.0 in stage 2.1 (TID 74) in 2328 ms on 10.0.0.10 (executor 22) (11/20)
17/08/29 13:14:46 INFO TaskSetManager: Starting task 13.0 in stage 2.1 (TID 77, 10.0.0.10, executor 22, partition 13, ANY, 4877 bytes)
17/08/29 13:14:46 INFO TaskSetManager: Finished task 11.0 in stage 2.1 (TID 75) in 2286 ms on 10.0.0.10 (executor 22) (12/20)
17/08/29 13:14:48 INFO TaskSetManager: Starting task 14.0 in stage 2.1 (TID 78, 10.0.0.10, executor 22, partition 14, ANY, 4877 bytes)
17/08/29 13:14:48 INFO TaskSetManager: Finished task 12.0 in stage 2.1 (TID 76) in 2272 ms on 10.0.0.10 (executor 22) (13/20)
17/08/29 13:14:48 INFO TaskSetManager: Starting task 15.0 in stage 2.1 (TID 79, 10.0.0.10, executor 22, partition 15, ANY, 4877 bytes)
17/08/29 13:14:48 INFO TaskSetManager: Finished task 13.0 in stage 2.1 (TID 77) in 2169 ms on 10.0.0.10 (executor 22) (14/20)
17/08/29 13:14:50 INFO TaskSetManager: Starting task 16.0 in stage 2.1 (TID 80, 10.0.0.10, executor 22, partition 16, ANY, 4877 bytes)
17/08/29 13:14:50 INFO TaskSetManager: Finished task 14.0 in stage 2.1 (TID 78) in 2243 ms on 10.0.0.10 (executor 22) (15/20)
17/08/29 13:14:50 INFO TaskSetManager: Starting task 17.0 in stage 2.1 (TID 81, 10.0.0.10, executor 22, partition 17, ANY, 4877 bytes)
17/08/29 13:14:50 INFO TaskSetManager: Finished task 15.0 in stage 2.1 (TID 79) in 2366 ms on 10.0.0.10 (executor 22) (16/20)
17/08/29 13:14:52 INFO TaskSetManager: Starting task 18.0 in stage 2.1 (TID 82, 10.0.0.10, executor 22, partition 18, ANY, 4877 bytes)
17/08/29 13:14:52 INFO TaskSetManager: Finished task 16.0 in stage 2.1 (TID 80) in 2336 ms on 10.0.0.10 (executor 22) (17/20)
17/08/29 13:14:53 INFO TaskSetManager: Starting task 19.0 in stage 2.1 (TID 83, 10.0.0.10, executor 22, partition 19, ANY, 4877 bytes)
17/08/29 13:14:53 INFO TaskSetManager: Finished task 17.0 in stage 2.1 (TID 81) in 2236 ms on 10.0.0.10 (executor 22) (18/20)
17/08/29 13:14:53 INFO TaskSetManager: Finished task 19.0 in stage 2.1 (TID 83) in 356 ms on 10.0.0.10 (executor 22) (19/20)
17/08/29 13:14:54 INFO TaskSetManager: Finished task 18.0 in stage 2.1 (TID 82) in 2114 ms on 10.0.0.10 (executor 22) (20/20)
17/08/29 13:14:54 INFO TaskSchedulerImpl: Removed TaskSet 2.1, whose tasks have all completed, from pool 
17/08/29 13:14:54 INFO DAGScheduler: ShuffleMapStage 2 (repartition at Word2Vec.scala:344) finished in 26.759 s
17/08/29 13:14:54 INFO DAGScheduler: looking for newly runnable stages
17/08/29 13:14:54 INFO DAGScheduler: running: Set()
17/08/29 13:14:54 INFO DAGScheduler: waiting: Set(ShuffleMapStage 3, ResultStage 4)
17/08/29 13:14:54 INFO DAGScheduler: failed: Set()
17/08/29 13:14:54 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[13] at mapPartitionsWithIndex at Word2Vec.scala:361), which has no missing parents
17/08/29 13:14:54 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 6.4 KB, free 84.8 MB)
17/08/29 13:14:54 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 3.5 KB, free 84.8 MB)
17/08/29 13:14:54 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.0.15:35768 (size: 3.5 KB, free: 266.0 MB)
17/08/29 13:14:54 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1006
17/08/29 13:14:54 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[13] at mapPartitionsWithIndex at Word2Vec.scala:361) (first 15 tasks are for partitions Vector(0))
17/08/29 13:14:54 INFO TaskSchedulerImpl: Adding task set 3.1 with 1 tasks
17/08/29 13:14:54 INFO TaskSetManager: Starting task 0.0 in stage 3.1 (TID 84, 10.0.0.10, executor 22, partition 0, NODE_LOCAL, 4890 bytes)
17/08/29 13:14:54 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.0.10:46456 (size: 3.5 KB, free: 1288.1 MB)
17/08/29 13:14:54 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.0.10:53390
17/08/29 13:14:54 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 166 bytes
17/08/29 13:14:59 INFO BlockManagerInfo: Added rdd_12_0 in memory on 10.0.0.10:46456 (size: 252.5 MB, free: 1035.7 MB)
17/08/29 13:14:59 INFO BlockManagerInfo: Added broadcast_6_piece16 in memory on 10.0.0.10:46456 (size: 4.0 MB, free: 1031.7 MB)
17/08/29 13:14:59 INFO BlockManagerInfo: Added broadcast_6_piece14 in memory on 10.0.0.10:46456 (size: 4.0 MB, free: 1027.7 MB)
17/08/29 13:14:59 INFO BlockManagerInfo: Added broadcast_6_piece10 in memory on 10.0.0.10:46456 (size: 4.0 MB, free: 1023.7 MB)
17/08/29 13:14:59 INFO BlockManagerInfo: Added broadcast_6_piece6 in memory on 10.0.0.10:46456 (size: 4.0 MB, free: 1019.7 MB)
17/08/29 13:14:59 INFO BlockManagerInfo: Added broadcast_6_piece5 in memory on 10.0.0.10:46456 (size: 4.0 MB, free: 1015.7 MB)
17/08/29 13:14:59 INFO BlockManagerInfo: Added broadcast_6_piece2 in memory on 10.0.0.10:46456 (size: 4.0 MB, free: 1011.7 MB)
17/08/29 13:14:59 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.0.10:46456 (size: 4.0 MB, free: 1007.7 MB)
17/08/29 13:14:59 INFO BlockManagerInfo: Added broadcast_6_piece12 in memory on 10.0.0.10:46456 (size: 4.0 MB, free: 1003.7 MB)
17/08/29 13:14:59 INFO BlockManagerInfo: Added broadcast_6_piece15 in memory on 10.0.0.10:46456 (size: 4.0 MB, free: 999.7 MB)
17/08/29 13:14:59 INFO BlockManagerInfo: Added broadcast_6_piece3 in memory on 10.0.0.10:46456 (size: 4.0 MB, free: 995.7 MB)
17/08/29 13:14:59 INFO BlockManagerInfo: Added broadcast_6_piece9 in memory on 10.0.0.10:46456 (size: 4.0 MB, free: 991.7 MB)
17/08/29 13:14:59 INFO BlockManagerInfo: Added broadcast_6_piece17 in memory on 10.0.0.10:46456 (size: 4.0 MB, free: 987.7 MB)
17/08/29 13:14:59 INFO BlockManagerInfo: Added broadcast_6_piece7 in memory on 10.0.0.10:46456 (size: 4.0 MB, free: 983.7 MB)
17/08/29 13:14:59 INFO BlockManagerInfo: Added broadcast_6_piece18 in memory on 10.0.0.10:46456 (size: 4.0 MB, free: 979.7 MB)
17/08/29 13:14:59 INFO BlockManagerInfo: Added broadcast_6_piece8 in memory on 10.0.0.10:46456 (size: 4.0 MB, free: 975.7 MB)
17/08/29 13:14:59 INFO BlockManagerInfo: Added broadcast_6_piece13 in memory on 10.0.0.10:46456 (size: 4.0 MB, free: 971.7 MB)
17/08/29 13:14:59 INFO BlockManagerInfo: Added broadcast_6_piece19 in memory on 10.0.0.10:46456 (size: 3.7 MB, free: 968.0 MB)
17/08/29 13:14:59 INFO BlockManagerInfo: Added broadcast_6_piece11 in memory on 10.0.0.10:46456 (size: 4.0 MB, free: 964.0 MB)
17/08/29 13:14:59 INFO BlockManagerInfo: Added broadcast_6_piece4 in memory on 10.0.0.10:46456 (size: 4.0 MB, free: 960.0 MB)
17/08/29 13:14:59 INFO BlockManagerInfo: Added broadcast_6_piece1 in memory on 10.0.0.10:46456 (size: 4.0 MB, free: 956.0 MB)
17/08/29 13:14:59 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.0.10:46456 (size: 398.1 KB, free: 955.6 MB)
17/08/29 13:14:59 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.0.10:46456 (size: 4.0 MB, free: 951.6 MB)
17/08/29 13:14:59 INFO BlockManagerInfo: Added broadcast_4_piece2 in memory on 10.0.0.10:46456 (size: 4.0 MB, free: 947.6 MB)
17/08/29 13:14:59 INFO BlockManagerInfo: Added broadcast_4_piece3 in memory on 10.0.0.10:46456 (size: 1002.6 KB, free: 946.6 MB)
17/08/29 13:14:59 INFO BlockManagerInfo: Added broadcast_4_piece1 in memory on 10.0.0.10:46456 (size: 4.0 MB, free: 942.6 MB)
17/08/29 13:15:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.0.10:46456 (size: 4.0 KB, free: 942.6 MB)
17/08/29 13:21:51 INFO TaskSetManager: Finished task 0.0 in stage 3.1 (TID 84) in 416842 ms on 10.0.0.10 (executor 22) (1/1)
17/08/29 13:21:51 INFO TaskSchedulerImpl: Removed TaskSet 3.1, whose tasks have all completed, from pool 
17/08/29 13:21:51 INFO DAGScheduler: ShuffleMapStage 3 (mapPartitionsWithIndex at Word2Vec.scala:361) finished in 416.842 s
17/08/29 13:21:51 INFO DAGScheduler: looking for newly runnable stages
17/08/29 13:21:51 INFO DAGScheduler: running: Set()
17/08/29 13:21:51 INFO DAGScheduler: waiting: Set(ResultStage 4)
17/08/29 13:21:51 INFO DAGScheduler: failed: Set()
17/08/29 13:21:51 INFO DAGScheduler: Submitting ResultStage 4 (ShuffledRDD[14] at reduceByKey at Word2Vec.scala:435), which has no missing parents
17/08/29 13:21:51 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 4.4 KB, free 84.8 MB)
17/08/29 13:21:51 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 2.4 KB, free 84.8 MB)
17/08/29 13:21:51 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.0.15:35768 (size: 2.4 KB, free: 266.0 MB)
17/08/29 13:21:51 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1006
17/08/29 13:21:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (ShuffledRDD[14] at reduceByKey at Word2Vec.scala:435) (first 15 tasks are for partitions Vector(0))
17/08/29 13:21:51 INFO TaskSchedulerImpl: Adding task set 4.1 with 1 tasks
17/08/29 13:21:51 INFO TaskSetManager: Starting task 0.0 in stage 4.1 (TID 85, 10.0.0.10, executor 22, partition 0, NODE_LOCAL, 4625 bytes)
17/08/29 13:21:51 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.0.10:46456 (size: 2.4 KB, free: 942.6 MB)
17/08/29 13:21:51 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.0.10:53390
17/08/29 13:21:51 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 138 bytes
17/08/29 13:21:57 INFO BlockManagerInfo: Added taskresult_85 in memory on 10.0.0.10:46456 (size: 169.8 MB, free: 772.8 MB)
17/08/29 13:21:57 INFO TransportClientFactory: Successfully created connection to /10.0.0.10:46456 after 1 ms (0 ms spent in bootstraps)
17/08/29 13:21:57 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.0.15:35768 in memory (size: 3.5 KB, free: 266.0 MB)
17/08/29 13:21:57 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.0.10:46456 in memory (size: 3.5 KB, free: 772.8 MB)
17/08/29 13:21:57 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.0.15:35768 in memory (size: 3.3 KB, free: 266.0 MB)
17/08/29 13:21:57 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.0.10:46456 in memory (size: 3.3 KB, free: 772.8 MB)
17/08/29 13:21:58 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.0.15:35768 in memory (size: 2.4 KB, free: 266.0 MB)
17/08/29 13:21:58 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.0.10:46456 in memory (size: 2.4 KB, free: 772.8 MB)
17/08/29 13:32:42 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=8857396640039381070, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]}} to /10.0.0.10:53390; closing connection
java.io.IOException: Broken pipe
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:65)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
	at org.apache.spark.network.protocol.MessageWithHeader.copyByteBuf(MessageWithHeader.java:142)
	at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:123)
	at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:287)
	at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237)
	at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:314)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:802)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0(AbstractNioChannel.java:313)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.flush(AbstractChannel.java:770)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.flush(DefaultChannelPipeline.java:1256)
	at io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:771)
	at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:763)
	at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:744)
	at io.netty.channel.ChannelOutboundHandlerAdapter.flush(ChannelOutboundHandlerAdapter.java:115)
	at io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:771)
	at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:763)
	at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:744)
	at io.netty.channel.ChannelDuplexHandler.flush(ChannelDuplexHandler.java:117)
	at io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:771)
	at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:763)
	at io.netty.channel.AbstractChannelHandlerContext.access$1500(AbstractChannelHandlerContext.java:35)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1117)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1051)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:399)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:446)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:748)
17/08/29 13:33:37 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=7916501869813178048, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]}} to /10.0.0.10:53390; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 13:33:41 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=7486894597158299462, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]}} to /10.0.0.10:53390; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 13:33:52 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=8408832763426185302, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]}} to /10.0.0.10:53390; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 13:34:44 WARN HeartbeatReceiver: Removing executor 22 with no recent heartbeats: 138255 ms exceeds timeout 120000 ms
17/08/29 13:34:45 ERROR TaskSchedulerImpl: Lost executor 22 on 10.0.0.10: Executor heartbeat timed out after 138255 ms
17/08/29 13:34:46 WARN TaskSetManager: Lost task 0.0 in stage 4.1 (TID 85, 10.0.0.10, executor 22): ExecutorLostFailure (executor 22 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 138255 ms
17/08/29 13:34:51 INFO StandaloneSchedulerBackend: Requesting to kill executor(s) 22
17/08/29 13:34:51 INFO DAGScheduler: Executor lost: 22 (epoch 9)
17/08/29 13:35:00 INFO BlockManagerMasterEndpoint: Trying to remove executor 22 from BlockManagerMaster.
17/08/29 13:35:05 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=6252563756115223811, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=47 cap=64]}} to /10.0.0.10:53440; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 13:35:17 WARN BlockManagerMasterEndpoint: No more replicas available for taskresult_85 !
17/08/29 13:35:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_12_0 !
17/08/29 13:35:24 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(22, 10.0.0.10, 46456, None)
17/08/29 13:35:31 INFO BlockManagerMaster: Removed 22 successfully in removeExecutor
17/08/29 13:35:38 INFO DAGScheduler: Shuffle files lost for executor: 22 (epoch 9)
17/08/29 13:35:43 INFO ShuffleMapStage: ShuffleMapStage 3 is now unavailable on executor 22 (0/1, false)
17/08/29 13:35:50 INFO ShuffleMapStage: ShuffleMapStage 2 is now unavailable on executor 22 (0/20, false)
17/08/29 13:35:57 INFO DAGScheduler: Host added was in lost list earlier: 10.0.0.10
17/08/29 13:36:02 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/22 is now EXITED (Command exited with code 56)
17/08/29 13:36:06 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/22 removed: Command exited with code 56
17/08/29 13:36:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/23 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 13:36:31 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/23 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 13:36:48 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/23 is now RUNNING
17/08/29 13:37:23 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/23 is now EXITED (Command exited with code 1)
17/08/29 13:37:29 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/23 removed: Command exited with code 1
17/08/29 13:37:36 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/24 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 13:37:43 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/24 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 13:37:45 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/24 is now RUNNING
17/08/29 13:37:55 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/24 is now EXITED (Command exited with code 1)
17/08/29 13:37:59 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/24 removed: Command exited with code 1
17/08/29 13:38:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/25 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 13:38:12 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/25 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 13:38:13 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/25 is now RUNNING
17/08/29 13:38:20 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 13:38:30 INFO TaskSetManager: Starting task 0.1 in stage 4.1 (TID 86, 10.0.0.10, executor 22, partition 0, NODE_LOCAL, 4625 bytes)
17/08/29 13:38:38 INFO StandaloneSchedulerBackend: Actual list of executor(s) to be killed is 22
17/08/29 13:39:48 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 13:40:05 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/25 is now EXITED (Command exited with code 1)
17/08/29 13:40:20 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/25 removed: Command exited with code 1
17/08/29 13:40:22 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 13:40:48 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/26 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 13:41:02 ERROR Utils: Uncaught exception in thread kill-executor-thread
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.killExecutors(CoarseGrainedSchedulerBackend.scala:637)
	at org.apache.spark.SparkContext.killAndReplaceExecutor(SparkContext.scala:1687)
	at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3$$anon$3$$anonfun$run$3.apply$mcV$sp(HeartbeatReceiver.scala:207)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317)
	at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3$$anon$3.run(HeartbeatReceiver.scala:204)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 10 more
17/08/29 13:41:10 WARN NettyRpcEnv: Ignored message: true
17/08/29 13:41:14 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/26 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 13:41:17 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/26 is now RUNNING
17/08/29 13:41:32 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=8145822251695482657, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=47 cap=64]}} to /10.0.0.10:53446; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 13:42:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/26 is now EXITED (Command exited with code 1)
17/08/29 13:42:08 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/26 removed: Command exited with code 1
17/08/29 13:42:19 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/27 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 13:42:30 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/27 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 13:42:37 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/27 is now RUNNING
17/08/29 13:42:39 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 13:42:49 ERROR TaskSchedulerImpl: Lost executor 22 on 10.0.0.10: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/08/29 13:42:49 WARN TaskSetManager: Lost task 0.1 in stage 4.1 (TID 86, 10.0.0.10, executor 22): ExecutorLostFailure (executor 22 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/08/29 13:43:00 INFO DAGScheduler: Executor lost: 22 (epoch 11)
17/08/29 13:43:02 INFO BlockManagerMasterEndpoint: Trying to remove executor 22 from BlockManagerMaster.
17/08/29 13:43:11 INFO BlockManagerMaster: Removed 22 successfully in removeExecutor
17/08/29 13:43:20 INFO DAGScheduler: Shuffle files lost for executor: 22 (epoch 11)
17/08/29 13:44:13 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/27 is now EXITED (Command exited with code 1)
17/08/29 13:44:13 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/27 removed: Command exited with code 1
17/08/29 13:44:18 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/28 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 13:44:23 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/28 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 13:44:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/28 is now RUNNING
17/08/29 13:44:33 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 13:45:48 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=7727012468176189362, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53442; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 13:46:13 INFO BlockManagerMaster: Removal of executor 22 requested
17/08/29 13:46:13 INFO BlockManagerMasterEndpoint: Trying to remove executor 22 from BlockManagerMaster.
17/08/29 13:46:13 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 22
17/08/29 13:46:15 WARN NettyRpcEnv: Ignored message: true
17/08/29 13:46:24 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 13:46:54 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/28 is now EXITED (Command exited with code 1)
17/08/29 13:46:59 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/28 removed: Command exited with code 1
17/08/29 13:47:07 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/29 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 13:47:23 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/29 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 13:47:28 INFO BlockManagerMaster: Removal of executor 23 requested
17/08/29 13:47:28 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=5480813536234908389, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53444; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 13:47:28 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/29 is now RUNNING
17/08/29 13:47:29 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 23
17/08/29 13:47:34 WARN NettyRpcEnv: Ignored message: true
17/08/29 13:47:35 INFO BlockManagerMasterEndpoint: Trying to remove executor 23 from BlockManagerMaster.
17/08/29 13:47:44 INFO BlockManagerMaster: Removal of executor 24 requested
17/08/29 13:47:44 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 24
17/08/29 13:47:44 WARN NettyRpcEnv: Ignored message: true
17/08/29 13:47:45 INFO BlockManagerMasterEndpoint: Trying to remove executor 24 from BlockManagerMaster.
17/08/29 13:49:18 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/29 is now EXITED (Command exited with code 1)
17/08/29 13:49:22 INFO BlockManagerMaster: Removal of executor 25 requested
17/08/29 13:49:22 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/29 removed: Command exited with code 1
17/08/29 13:49:23 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 25
17/08/29 13:49:25 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/30 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 13:49:27 WARN NettyRpcEnv: Ignored message: true
17/08/29 13:49:37 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 13:49:37 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/30 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 13:50:10 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/30 is now RUNNING
17/08/29 13:50:34 INFO BlockManagerMasterEndpoint: Trying to remove executor 25 from BlockManagerMaster.
17/08/29 13:51:04 INFO BlockManagerMaster: Removal of executor 26 requested
17/08/29 13:51:07 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 26
17/08/29 13:51:07 WARN NettyRpcEnv: Ignored message: true
17/08/29 13:51:07 INFO BlockManagerMasterEndpoint: Trying to remove executor 26 from BlockManagerMaster.
17/08/29 13:51:35 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 13:52:54 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=5592060434703124423, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53448; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 13:54:09 INFO BlockManagerMaster: Removal of executor 27 requested
17/08/29 13:54:09 INFO BlockManagerMasterEndpoint: Trying to remove executor 27 from BlockManagerMaster.
17/08/29 13:54:14 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 27
17/08/29 13:54:21 WARN NettyRpcEnv: Ignored message: true
17/08/29 13:54:59 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=9073141752579177944, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53450; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 13:56:24 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=7615220407973089302, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53452; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 13:56:35 INFO BlockManagerMasterEndpoint: Trying to remove executor 28 from BlockManagerMaster.
17/08/29 13:56:38 INFO BlockManagerMaster: Removal of executor 28 requested
17/08/29 13:56:38 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 28
17/08/29 13:56:38 WARN NettyRpcEnv: Ignored message: true
17/08/29 13:56:40 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/30 is now EXITED (Command exited with code 1)
17/08/29 13:56:54 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/30 removed: Command exited with code 1
17/08/29 13:57:41 INFO BlockManagerMaster: Removal of executor 29 requested
17/08/29 13:57:45 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 29
17/08/29 13:57:43 INFO BlockManagerMasterEndpoint: Trying to remove executor 29 from BlockManagerMaster.
17/08/29 13:57:46 WARN NettyRpcEnv: Ignored message: true
17/08/29 13:57:54 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/31 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 13:58:04 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/31 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 13:59:30 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 13:59:32 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=7857793821786802215, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53454; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 14:01:13 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=8740282093152066921, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53456; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 14:01:48 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/31 is now RUNNING
17/08/29 14:02:39 INFO BlockManagerMaster: Removal of executor 30 requested
17/08/29 14:02:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 30
17/08/29 14:02:41 INFO BlockManagerMasterEndpoint: Trying to remove executor 30 from BlockManagerMaster.
17/08/29 14:02:44 WARN NettyRpcEnv: Ignored message: true
17/08/29 14:03:02 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=6169505077809726841, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53458; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 14:03:27 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=7054695830194164763, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=47 cap=64]}} to /10.0.0.10:53462; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 14:04:42 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/31 is now EXITED (Command exited with code 1)
17/08/29 14:04:53 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/31 removed: Command exited with code 1
17/08/29 14:04:59 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=6336664554275293738, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53460; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 14:05:36 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/32 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 14:05:48 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/32 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 14:05:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/32 is now RUNNING
17/08/29 14:06:15 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/32 is now EXITED (Command exited with code 1)
17/08/29 14:06:15 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/32 removed: Command exited with code 1
17/08/29 14:06:16 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/33 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 14:06:23 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/33 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 14:06:37 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/33 is now RUNNING
17/08/29 14:06:51 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/33 is now EXITED (Command exited with code 1)
17/08/29 14:06:51 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/33 removed: Command exited with code 1
17/08/29 14:07:01 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/34 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 14:07:04 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 14:07:05 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/34 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 14:07:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/34 is now RUNNING
17/08/29 14:07:37 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/34 is now EXITED (Command exited with code 1)
17/08/29 14:07:41 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/34 removed: Command exited with code 1
17/08/29 14:07:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/35 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 14:07:53 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/35 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 14:07:54 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/35 is now RUNNING
17/08/29 14:08:33 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/35 is now EXITED (Command exited with code 1)
17/08/29 14:08:34 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/35 removed: Command exited with code 1
17/08/29 14:08:35 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 14:08:37 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/36 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 14:08:53 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 14:09:07 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/36 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 14:09:31 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/36 is now RUNNING
17/08/29 14:09:37 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/36 is now EXITED (Command exited with code 1)
17/08/29 14:09:40 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/36 removed: Command exited with code 1
17/08/29 14:09:47 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/37 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 14:10:02 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/37 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 14:10:02 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 14:10:02 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/37 is now RUNNING
17/08/29 14:10:30 INFO BlockManagerMaster: Removal of executor 31 requested
17/08/29 14:10:30 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 31
17/08/29 14:10:30 WARN NettyRpcEnv: Ignored message: true
17/08/29 14:10:33 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=6216486820346612914, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53464; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 14:10:54 INFO SparkContext: Invoking stop() from shutdown hook
17/08/29 14:10:58 INFO BlockManagerMasterEndpoint: Trying to remove executor 31 from BlockManagerMaster.
17/08/29 14:11:06 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 14:11:39 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/37 is now EXITED (Command exited with code 1)
17/08/29 14:11:29 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=7541869581587174730, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=47 cap=64]}} to /10.0.0.10:53468; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 14:11:48 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 14:11:46 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/37 removed: Command exited with code 1
17/08/29 14:11:52 INFO BlockManagerMaster: Removal of executor 32 requested
17/08/29 14:12:06 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/38 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 14:12:07 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/38 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 14:12:08 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/38 is now RUNNING
17/08/29 14:12:08 INFO BlockManagerMasterEndpoint: Trying to remove executor 32 from BlockManagerMaster.
17/08/29 14:12:08 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 32
17/08/29 14:12:08 WARN NettyRpcEnv: Ignored message: true
17/08/29 14:12:44 INFO BlockManagerMaster: Removal of executor 33 requested
17/08/29 14:12:48 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 33
17/08/29 14:12:48 INFO AbstractConnector: Stopped Spark@352c308{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17/08/29 14:12:47 INFO BlockManagerMasterEndpoint: Trying to remove executor 33 from BlockManagerMaster.
17/08/29 14:12:48 WARN NettyRpcEnv: Ignored message: true
17/08/29 14:13:55 INFO SparkUI: Stopped Spark web UI at http://10.0.0.15:4040
17/08/29 14:14:07 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 14:14:30 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=5745978541374961758, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53466; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 14:14:32 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/38 is now EXITED (Command exited with code 1)
17/08/29 14:14:34 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/38 removed: Command exited with code 1
17/08/29 14:14:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/39 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 14:14:52 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/39 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 14:15:13 INFO BlockManagerMaster: Removal of executor 34 requested
17/08/29 14:15:13 INFO BlockManagerMasterEndpoint: Trying to remove executor 34 from BlockManagerMaster.
17/08/29 14:15:13 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 34
17/08/29 14:15:15 WARN NettyRpcEnv: Ignored message: true
17/08/29 14:15:18 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/39 is now RUNNING
17/08/29 14:15:47 INFO BlockManagerMaster: Removal of executor 35 requested
17/08/29 14:15:47 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 35
17/08/29 14:15:51 INFO BlockManagerMasterEndpoint: Trying to remove executor 35 from BlockManagerMaster.
17/08/29 14:15:55 WARN NettyRpcEnv: Ignored message: true
17/08/29 14:16:43 INFO BlockManagerMaster: Removal of executor 36 requested
17/08/29 14:16:43 INFO BlockManagerMasterEndpoint: Trying to remove executor 36 from BlockManagerMaster.
17/08/29 14:16:43 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 14:16:43 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 36
17/08/29 14:17:07 WARN NettyRpcEnv: Ignored message: true
17/08/29 14:19:13 INFO BlockManagerMaster: Removal of executor 37 requested
17/08/29 14:19:13 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 37
17/08/29 14:19:22 WARN NettyRpcEnv: Ignored message: true
17/08/29 14:19:33 INFO BlockManagerMasterEndpoint: Trying to remove executor 37 from BlockManagerMaster.
17/08/29 14:19:51 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=5943938531093809753, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=47 cap=64]}} to /10.0.0.10:53474; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 14:20:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/39 is now EXITED (Command exited with code 1)
17/08/29 14:20:21 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/39 removed: Command exited with code 1
17/08/29 14:20:40 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/40 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 14:20:55 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/40 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 14:21:23 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=9080965227387963374, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53470; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 14:21:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/40 is now RUNNING
17/08/29 14:21:33 INFO DAGScheduler: ResultStage 4 (collect at Word2Vec.scala:438) failed in 3574.221 s due to Stage cancelled because SparkContext was shut down
17/08/29 14:21:34 INFO DAGScheduler: Job 1 failed: collect at Word2Vec.scala:438, took 8506.354352 s
17/08/29 14:21:37 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/40 is now EXITED (Command exited with code 1)
17/08/29 14:21:37 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/40 removed: Command exited with code 1
17/08/29 14:21:49 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/41 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 14:21:50 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@27d9ac50)
17/08/29 14:21:52 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/41 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 14:21:59 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/41 is now RUNNING
17/08/29 14:22:10 INFO BlockManagerMaster: Removal of executor 38 requested
17/08/29 14:22:10 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 38
17/08/29 14:22:10 WARN NettyRpcEnv: Ignored message: true
17/08/29 14:22:11 INFO BlockManagerMasterEndpoint: Trying to remove executor 38 from BlockManagerMaster.
17/08/29 14:22:13 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(1,1503991313064,JobFailed(org.apache.spark.SparkException: Job 1 cancelled because SparkContext was shut down))
17/08/29 14:22:16 INFO TorrentBroadcast: Destroying Broadcast(3) (from destroy at Word2Vec.scala:321)
17/08/29 14:22:25 INFO StandaloneSchedulerBackend: Shutting down all executors
17/08/29 14:22:25 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 14:22:47 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/41 is now EXITED (Command exited with code 1)
17/08/29 14:22:50 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/41 removed: Command exited with code 1
17/08/29 14:22:51 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/42 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 14:22:53 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=7734567786140312192, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53472; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 14:22:58 INFO TorrentBroadcast: Destroying Broadcast(4) (from destroy at Word2Vec.scala:322)
17/08/29 14:22:53 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/42 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 14:23:08 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/42 is now RUNNING
17/08/29 14:23:21 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.0.15:35768 on disk (size: 4.0 KB)
17/08/29 14:23:29 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.0.0.15, 35768, None),broadcast_3_piece0,StorageLevel(1 replicas),0,0))
17/08/29 14:23:47 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 14:23:54 INFO TorrentBroadcast: Destroying Broadcast(5) (from destroy at Word2Vec.scala:323)
Exception in thread "main" org.apache.spark.SparkException: Job 1 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1732)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1651)
	at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1921)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1920)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:935)
	at org.apache.spark.mllib.feature.Word2Vec$$anonfun$doFit$1.apply$mcVI$sp(Word2Vec.scala:438)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.mllib.feature.Word2Vec.doFit(Word2Vec.scala:358)
	at org.apache.spark.mllib.feature.Word2Vec.fit(Word2Vec.scala:319)
	at W2V$.main(W2V.scala:13)
	at W2V.main(W2V.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
17/08/29 14:24:22 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.0.15:35768 in memory (size: 4.0 MB, free: 270.0 MB)
17/08/29 14:24:51 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 14:24:46 ERROR Utils: Uncaught exception in thread Thread-2
org.apache.spark.SparkException: Error asking standalone scheduler to shut down executors
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.stopExecutors(CoarseGrainedSchedulerBackend.scala:398)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.stop(CoarseGrainedSchedulerBackend.scala:403)
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.org$apache$spark$scheduler$cluster$StandaloneSchedulerBackend$$stop(StandaloneSchedulerBackend.scala:219)
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.stop(StandaloneSchedulerBackend.scala:123)
	at org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:517)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1652)
	at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1921)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1920)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
Caused by: org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:76)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.stopExecutors(CoarseGrainedSchedulerBackend.scala:394)
	... 21 more
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 24 more
17/08/29 14:25:21 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.0.0.15, 35768, None),broadcast_4_piece0,StorageLevel(1 replicas),0,0))
17/08/29 14:26:09 INFO BlockManagerInfo: Removed broadcast_4_piece3 on 10.0.0.15:35768 in memory (size: 1002.6 KB, free: 271.0 MB)
17/08/29 14:26:18 WARN BlockManagerMaster: Failed to remove broadcast 4 with removeFromMaster = true - Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 14:26:26 INFO BlockManagerMaster: Removal of executor 39 requested
17/08/29 14:26:19 WARN BlockManagerMaster: Failed to remove broadcast 5 with removeFromMaster = true - Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 14:26:19 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.0.0.15, 35768, None),broadcast_4_piece3,StorageLevel(1 replicas),0,0))
17/08/29 14:26:31 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 39
17/08/29 14:26:48 INFO BlockManagerInfo: Removed broadcast_4_piece2 on 10.0.0.15:35768 in memory (size: 4.0 MB, free: 275.0 MB)
17/08/29 14:26:48 WARN NettyRpcEnv: Ignored message: true
17/08/29 14:27:07 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.0.0.15, 35768, None),broadcast_4_piece2,StorageLevel(1 replicas),0,0))
17/08/29 14:27:09 INFO BlockManagerMasterEndpoint: Trying to remove executor 39 from BlockManagerMaster.
17/08/29 14:27:24 INFO BlockManagerInfo: Removed broadcast_4_piece1 on 10.0.0.15:35768 in memory (size: 4.0 MB, free: 279.0 MB)
17/08/29 14:27:38 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.0.0.15, 35768, None),broadcast_4_piece1,StorageLevel(1 replicas),0,0))
17/08/29 14:27:40 WARN NettyRpcEnv: Ignored message: 0
17/08/29 14:27:40 WARN NettyRpcEnv: Ignored message: true
17/08/29 14:27:41 WARN NettyRpcEnv: Ignored message: true
17/08/29 14:28:22 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=6091030315390783431, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=47 cap=64]}} to /10.0.0.10:53480; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 14:28:23 INFO BlockManagerInfo: Removed broadcast_5_piece1 on 10.0.0.15:35768 in memory (size: 3.2 MB, free: 282.2 MB)
17/08/29 14:28:24 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.0.0.15, 35768, None),broadcast_5_piece1,StorageLevel(1 replicas),0,0))
17/08/29 14:28:25 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/42 is now EXITED (Command exited with code 1)
17/08/29 14:28:25 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/42 removed: Command exited with code 1
17/08/29 14:28:25 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/43 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 14:28:27 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/43 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 14:28:27 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/43 is now RUNNING
17/08/29 14:28:27 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/43 is now EXITED (Command exited with code 1)
17/08/29 14:28:28 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/43 removed: Command exited with code 1
17/08/29 14:28:28 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/44 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 14:28:30 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/44 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 14:28:30 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/44 is now RUNNING
17/08/29 14:28:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/08/29 14:28:45 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.0.15:35768 in memory (size: 4.0 MB, free: 286.2 MB)
17/08/29 14:28:45 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.0.0.15, 35768, None),broadcast_5_piece0,StorageLevel(1 replicas),0,0))
17/08/29 14:28:57 WARN NettyRpcEnv: Ignored message: 0
17/08/29 14:29:27 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=7154900596379358251, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53476; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 14:29:56 INFO BlockManagerMaster: Removal of executor 40 requested
17/08/29 14:29:56 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 40
17/08/29 14:29:56 WARN NettyRpcEnv: Ignored message: true
17/08/29 14:29:57 INFO BlockManagerMasterEndpoint: Trying to remove executor 40 from BlockManagerMaster.
17/08/29 14:30:47 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 14:30:54 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
17/08/29 14:30:55 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 14:31:08 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/44 is now EXITED (Command exited with code 1)
17/08/29 14:31:09 WARN NettyRpcEnv: Ignored message: true
17/08/29 14:31:18 INFO StandaloneSchedulerBackend: Executor app-20170829115831-0139/44 removed: Command exited with code 1
17/08/29 14:31:31 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170829115831-0139/45 on worker-20170822112324-10.0.0.10-45253 (10.0.0.10:45253) with 2 cores
17/08/29 14:31:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20170829115831-0139/45 on hostPort 10.0.0.10:45253 with 2 cores, 2.7 GB RAM
17/08/29 14:31:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170829115831-0139/45 is now RUNNING
17/08/29 14:31:59 INFO BlockManagerMaster: Removal of executor 41 requested
17/08/29 14:31:59 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 41
17/08/29 14:32:00 INFO BlockManagerMasterEndpoint: Trying to remove executor 41 from BlockManagerMaster.
17/08/29 14:32:00 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=8881989216620544597, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53478; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 14:32:01 WARN NettyRpcEnv: Ignored message: true
17/08/29 14:33:55 ERROR StandaloneSchedulerBackend: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.0.15:44957 in 120 seconds
	... 8 more
17/08/29 14:35:14 INFO BlockManagerMaster: Removal of executor 42 requested
17/08/29 14:35:15 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 42
17/08/29 14:35:18 INFO BlockManagerMasterEndpoint: Trying to remove executor 42 from BlockManagerMaster.
17/08/29 14:35:19 WARN NettyRpcEnv: Ignored message: true
17/08/29 14:35:21 INFO BlockManagerMaster: Removal of executor 43 requested
17/08/29 14:35:21 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 43
17/08/29 14:35:21 WARN NettyRpcEnv: Ignored message: true
17/08/29 14:35:21 INFO BlockManagerMasterEndpoint: Trying to remove executor 43 from BlockManagerMaster.
17/08/29 14:35:21 INFO MemoryStore: MemoryStore cleared
17/08/29 14:35:21 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=6375720348911414985, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53482; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 14:35:21 INFO BlockManagerMasterEndpoint: Trying to remove executor 44 from BlockManagerMaster.
17/08/29 14:35:21 INFO BlockManagerMaster: Removal of executor 44 requested
17/08/29 14:35:21 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 44
17/08/29 14:35:21 WARN NettyRpcEnv: Ignored message: true
17/08/29 14:35:21 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=6002422805776151468, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=1272 cap=1400]}} to /10.0.0.10:53484; closing connection
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/08/29 14:35:21 INFO BlockManager: BlockManager stopped
17/08/29 14:35:21 INFO BlockManagerMaster: BlockManagerMaster stopped
17/08/29 14:35:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/08/29 14:35:21 INFO SparkContext: Successfully stopped SparkContext
17/08/29 14:35:21 INFO ShutdownHookManager: Shutdown hook called
17/08/29 14:35:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-41a1861b-dd82-456e-8ac0-f222dfef0290
